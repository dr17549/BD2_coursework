{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import optuna\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(999)\n",
    "from tensorflow.keras.regularizers import L1\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,BatchNormalization,Activation\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "\n",
    "panel = pd.read_pickle('../Data/returns_chars_panel.pkl') \n",
    "macro = pd.read_pickle('../Data/macro_timeseries.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine micro and macro data\n",
    "df = pd.merge(panel,macro,on='date',how='left',suffixes=['','_macro']) \n",
    "\n",
    "# features + targets \n",
    "X = df.drop(columns=['ret','excess_ret','rfree','permno','date']) # everything except return info and IDs\n",
    "y = df['excess_ret'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Train Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make 20 years of training data\n",
    "date = df['date']\n",
    "training = (date <= '1977-03') # selects \n",
    "X_train, y_train = X.loc[training].values, y.loc[training].values \n",
    "\n",
    "# make 10 years of validation data\n",
    "validation = (date > '1977-03') & (date <= '1987-03') \n",
    "X_val, y_val = X.loc[validation].values, y.loc[validation].values \n",
    "\n",
    "# make test data\n",
    "test = (date > '1987-03') \n",
    "X_test, y_test = X.loc[test].values, y.loc[test].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lamda = 1e-5\n",
    "epochs = 100\n",
    "# learning_rate = 0.0001\n",
    "patience = 5\n",
    "batch_size = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding baseline model for Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the final model \n",
    "def create_nn(n_layers, input_dim, lamda, learning_rate):\n",
    "    num_layers = 32 \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_dim=input_dim, \n",
    "                kernel_regularizer=regularizers.L1(lamda), \n",
    "                kernel_initializer = 'he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # add extra hidden layers \n",
    "    for i in range(n_layers - 1): \n",
    "        num_layers = int(num_layers / 2)\n",
    "        print(num_layers)\n",
    "        model.add(Dense(num_layers,\n",
    "                kernel_regularizer=regularizers.L1(lamda), \n",
    "                kernel_initializer = 'he_normal'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "    # output layer \n",
    "    model.add(Dense(1, activation='linear',\n",
    "                    kernel_regularizer=regularizers.L1(0.01), \n",
    "                    kernel_initializer = 'he_normal'))\n",
    "\n",
    "    model.compile(loss='mse', \n",
    "                optimizer=optimizer,\n",
    "                metrics = ['mse']) \n",
    "    return model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation for Lamda for L2 Regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 10:18:42,183] A new study created in memory with name: no-name-4ea5b825-f68d-477d-873a-ea743ec109b7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 10:19:06,213] Trial 0 finished with value: 0.026645848527550697 and parameters: {'learning_rate': 0.001, 'l1_reg': 0.0005436366706974076}. Best is trial 0 with value: 0.026645848527550697.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 10:19:52,434] Trial 1 finished with value: 0.02643212489783764 and parameters: {'learning_rate': 0.001, 'l1_reg': 3.274073920839725e-05}. Best is trial 1 with value: 0.02643212489783764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 10:20:18,923] Trial 2 finished with value: 0.026593651622533798 and parameters: {'learning_rate': 0.001, 'l1_reg': 0.0001586046427887883}. Best is trial 1 with value: 0.02643212489783764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 10:20:42,599] Trial 3 finished with value: 0.028688626363873482 and parameters: {'learning_rate': 0.01, 'l1_reg': 0.0005548669578422475}. Best is trial 1 with value: 0.02643212489783764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 10:21:07,792] Trial 4 finished with value: 0.027741363272070885 and parameters: {'learning_rate': 0.01, 'l1_reg': 0.0003561459577263908}. Best is trial 1 with value: 0.02643212489783764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 10:21:42,681] Trial 5 finished with value: 0.02658342570066452 and parameters: {'learning_rate': 0.001, 'l1_reg': 0.0003823760995679974}. Best is trial 1 with value: 0.02643212489783764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 10:22:31,030] Trial 6 finished with value: 0.026645097881555557 and parameters: {'learning_rate': 0.001, 'l1_reg': 7.585546665114775e-05}. Best is trial 1 with value: 0.02643212489783764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 10:23:08,125] Trial 7 finished with value: 0.026483960449695587 and parameters: {'learning_rate': 0.001, 'l1_reg': 3.6002518380953646e-05}. Best is trial 1 with value: 0.02643212489783764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 10:23:50,436] Trial 8 finished with value: 0.02637159638106823 and parameters: {'learning_rate': 0.001, 'l1_reg': 1.6762375688023723e-05}. Best is trial 8 with value: 0.02637159638106823.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 10:24:19,073] Trial 9 finished with value: 0.02843630127608776 and parameters: {'learning_rate': 0.01, 'l1_reg': 0.000497867338072132}. Best is trial 8 with value: 0.02637159638106823.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  Value: 0.02637159638106823\n",
      "  Params: \n",
      "    learning_rate: 0.001\n",
      "    l1_reg: 1.6762375688023723e-05\n"
     ]
    }
   ],
   "source": [
    "# Using Optuna to cross validate hyper parameter \n",
    "input_dim = X_train.shape[1]\n",
    "n_layers = 4\n",
    "def create_model(trial):\n",
    "\n",
    "    num_layers = 32 \n",
    "    # Suggest hyperparameters\n",
    "    learning_rate = trial.suggest_categorical('learning_rate', [0.001, 0.01])\n",
    "    l1_reg = trial.suggest_float('l1_reg', 1e-5, 1e-3, log=True)\n",
    "\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_dim=input_dim, \n",
    "                kernel_regularizer=regularizers.L1(l1_reg), \n",
    "                kernel_initializer = 'he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # add extra hidden layers \n",
    "    for i in range(n_layers - 1): \n",
    "        num_layers = int(num_layers / 2)\n",
    "        model.add(Dense(num_layers,\n",
    "                kernel_regularizer=regularizers.L1(l1_reg), \n",
    "                kernel_initializer = 'he_normal'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "    # output layer \n",
    "    model.add(Dense(1, activation='linear',\n",
    "                    kernel_regularizer=regularizers.L1(0.01), \n",
    "                    kernel_initializer = 'he_normal'))\n",
    "\n",
    "    model.compile(loss='mse', \n",
    "                optimizer=optimizer,\n",
    "                metrics = ['mse']) \n",
    "    return model\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial):\n",
    "    model = create_model(trial)\n",
    "    \n",
    "    # Use early stopping\n",
    "    early_stopping = EarlyStopping(patience=patience, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs,\n",
    "        batch_size,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Evaluate the model\n",
    "    score = model.evaluate(X_val, y_val, verbose=0)\n",
    "    return score[0]\n",
    "\n",
    "# Create a study and optimize the objective function\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Get the best trial\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(\"Best trial:\")\n",
    "print(f\"  Value: {best_trial.value}\")\n",
    "print(\"  Params: \")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Hyperparameters \n",
    "## NN2 \n",
    "- learning_rate = 0.001 \n",
    "- l1_reg = 1.76e-05\n",
    "\n",
    "## NN3 \n",
    "- learning_rate = 0.001 \n",
    "- lamda = 2.91e-05\n",
    "\n",
    "## N4 \n",
    "- learning_rate = 0.001 \n",
    "- l1_reg = 1.67e-05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 : Evaluate R_2_OOS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_R_2_OOS = [] \n",
    "for i in range(1977, 1987):\n",
    "    train_window = (date < datetime(i+1,1,1)) \n",
    "    test_window = (date >= datetime(i,1,1)) & (date < datetime(i+1,1,1)) \n",
    "    # print('Train', np.sum(train_window))\n",
    "    # print('Test', np.sum(test_window))\n",
    "    X_train_expanding, y_train_expanding = X.loc[train_window].values, y.loc[train_window].values\n",
    "    X_test_expanding, y_test_expanding =  X.loc[test_window].values, y.loc[test_window].values\n",
    "\n",
    "    lamda = 1e-5\n",
    "    epochs = 100\n",
    "    learning_rate = 0.0001\n",
    "    patience = 5\n",
    "    batch_size = 10000\n",
    "\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model = create_model(lamda)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mse'])\n",
    "    history = model.fit(X_train_expanding, y_train_expanding, \n",
    "                        epochs=epochs, \n",
    "                        batch_size=batch_size, \n",
    "                        verbose=True,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        callbacks=[EarlyStopping(patience=patience, restore_best_weights=True)])\n",
    "\n",
    "    # Plotting the training and validation MSE\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    pd.Series(history.history['mse']).plot(ax=ax, label='training mse')\n",
    "    pd.Series(history.history['val_mse']).plot(ax=ax, label='validation mse')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "    predictions = model.predict(X_val)\n",
    "    print(predictions)\n",
    "    df_predictions = pd.DataFrame(predictions, columns=['Prediction'])\n",
    "    df_predictions['Actual'] = y_val\n",
    "    df_predictions['dif_squared'] = (df_predictions['Prediction'] - df_predictions['Actual'])**2\n",
    "    df_predictions['actual_sqaured'] = df_predictions['Actual']**2\n",
    "    R_OOS = 1 - (df_predictions['dif_squared'].sum()/df_predictions['actual_sqaured'].sum()) \n",
    "    R_OOS\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expanding Window R^2_OOS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhammatornriewcharoon/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot perform 'rand_' with a dtyped [bool] array and scalar of type [bool]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:305\u001b[0m, in \u001b[0;36mna_logical_op\u001b[0;34m(x, y, op)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;66;03m# For exposition, write:\u001b[39;00m\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;66;03m#  yarr = isinstance(y, np.ndarray)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;66;03m# Then Cases where this goes through without raising include:\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;66;03m#  (xint or xbool) and (yint or bool)\u001b[39;00m\n\u001b[0;32m--> 305\u001b[0m     result \u001b[38;5;241m=\u001b[39m op(x, y)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/roperator.py:54\u001b[0m, in \u001b[0;36mrand_\u001b[0;34m(left, right)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrand_\u001b[39m(left, right):\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m operator\u001b[38;5;241m.\u001b[39mand_(right, left)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for &: 'datetime.datetime' and 'bool'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:319\u001b[0m, in \u001b[0;36mna_logical_op\u001b[0;34m(x, y, op)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m     result \u001b[38;5;241m=\u001b[39m libops\u001b[38;5;241m.\u001b[39mscalar_binop(x, y, op)\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;167;01mTypeError\u001b[39;00m,\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;167;01mValueError\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;167;01mNotImplementedError\u001b[39;00m,\n\u001b[1;32m    326\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/ops.pyx:180\u001b[0m, in \u001b[0;36mpandas._libs.ops.scalar_binop\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Buffer dtype mismatch, expected 'Python object' but got 'bool'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m validation_window \u001b[38;5;241m=\u001b[39m (date \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m datetime(i,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m&\u001b[39m (date \u001b[38;5;241m<\u001b[39m datetime(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)) \n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# make test data\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m test_window \u001b[38;5;241m=\u001b[39m (date \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m datetime(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m (date \u001b[38;5;241m<\u001b[39m datetime(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m17\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m))) \n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Window : \u001b[39m\u001b[38;5;124m\"\u001b[39m, datetime(i,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVal Window : \u001b[39m\u001b[38;5;124m\"\u001b[39m, datetime(i,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m) , \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m : \u001b[39m\u001b[38;5;124m\"\u001b[39m , datetime(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/ops/common.py:72\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     70\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m method(\u001b[38;5;28mself\u001b[39m, other)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/arraylike.py:76\u001b[0m, in \u001b[0;36mOpsMixin.__rand__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__rand__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__rand__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logical_method(other, roperator\u001b[38;5;241m.\u001b[39mrand_)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/series.py:6254\u001b[0m, in \u001b[0;36mSeries._logical_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6251\u001b[0m lvalues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m   6252\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 6254\u001b[0m res_values \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mlogical_op(lvalues, rvalues, op)\n\u001b[1;32m   6255\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(res_values, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:395\u001b[0m, in \u001b[0;36mlogical_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;66;03m# For int vs int `^`, `|`, `&` are bitwise operators and return\u001b[39;00m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;66;03m#   integer dtypes.  Otherwise these are boolean ops\u001b[39;00m\n\u001b[1;32m    393\u001b[0m filler \u001b[38;5;241m=\u001b[39m fill_int \u001b[38;5;28;01mif\u001b[39;00m is_self_int_dtype \u001b[38;5;129;01mand\u001b[39;00m is_other_int_dtype \u001b[38;5;28;01melse\u001b[39;00m fill_bool\n\u001b[0;32m--> 395\u001b[0m res_values \u001b[38;5;241m=\u001b[39m na_logical_op(lvalues, rvalues, op)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;66;03m# error: Cannot call function of unknown type\u001b[39;00m\n\u001b[1;32m    397\u001b[0m res_values \u001b[38;5;241m=\u001b[39m filler(res_values)  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:328\u001b[0m, in \u001b[0;36mna_logical_op\u001b[0;34m(x, y, op)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    321\u001b[0m             \u001b[38;5;167;01mTypeError\u001b[39;00m,\n\u001b[1;32m    322\u001b[0m             \u001b[38;5;167;01mValueError\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[38;5;167;01mNotImplementedError\u001b[39;00m,\n\u001b[1;32m    326\u001b[0m         ) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    327\u001b[0m             typ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(y)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m--> 328\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    329\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot perform \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m with a dtyped [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] array \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    330\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand scalar of type [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtyp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    331\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot perform 'rand_' with a dtyped [bool] array and scalar of type [bool]"
     ]
    }
   ],
   "source": [
    "end_year = date.max()\n",
    "\n",
    "lamda = 1e-05\n",
    "epochs = 100\n",
    "learning_rate = 0.001\n",
    "patience = 5\n",
    "batch_size = 10000\n",
    "#TODO \n",
    "# Change hidden layers here \n",
    "hidden_layers = 2\n",
    "\n",
    "total_R_2_OOS = [] \n",
    "model = create_nn(hidden_layers, X_test.shape[1], lamda , learning_rate)\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "\n",
    "for i in range(1988, end_year.year - 12 - 5):\n",
    "    # train_window = (date > datetime(1987,3,1)) & (date < datetime(i,1,1)) \n",
    "    # test_window = (date >= datetime(i,1,1)) & (date < datetime(i+1,1,1)) \n",
    "    # print(\"Train Window : \", datetime(1987,3,1),\" : \", datetime(i,1,1))\n",
    "    # print(\"Test Window : \", datetime(i,1,1) , \" : \" , datetime(i+1,1,1))\n",
    "\n",
    "    training_window = (date < datetime(i,1,1)) # selects \n",
    "    # make 12 years of validation data\n",
    "    validation_window = (date >= datetime(i,1,1)) & (date < datetime(i+12,1,1)) \n",
    "    # make test data\n",
    "    test_window = (date >= datetime(i+12,1,1)) & (date < datetime(i+17,1,1))\n",
    "    # test_window = (date >= datetime(i+12,1,1) & (date < datetime(i+17,1,1))) \n",
    "\n",
    "    print(\"Train Window : \", datetime(i,1,1))\n",
    "    print(\"Val Window : \", datetime(i,1,1) , \" : \" , datetime(i+12,1,1))\n",
    "    print(\"Test Window : \", datetime(i+12,1,1))\n",
    "\n",
    "    X_train_expanding, y_train_expanding = X.loc[training_window].values, y.loc[training_window].values\n",
    "    X_val_expanding , y_val_expanding = X.loc[validation_window].values, y.loc[validation_window].values\n",
    "    X_test_expanding, y_test_expanding =  X.loc[test_window].values, y.loc[test_window].values\n",
    "\n",
    "    # model.summary()\n",
    "    history = model.fit(X_train_expanding, y_train_expanding, \n",
    "                        epochs=100, \n",
    "                        batch_size=batch_size, \n",
    "                        verbose=False,\n",
    "                        validation_data = (X_val_expanding, y_val_expanding),\n",
    "                        callbacks = [EarlyStopping(patience = patience, restore_best_weights=True)])\n",
    "    predictions = model.predict(X_test_expanding)\n",
    "    df_predictions = pd.DataFrame(predictions, columns=['Prediction'])\n",
    "    df_predictions['Actual'] = y_test_expanding\n",
    "    df_predictions['dif_squared'] = (df_predictions['Prediction'] - df_predictions['Actual'])**2\n",
    "    df_predictions['actual_sqaured'] = df_predictions['Actual']**2\n",
    "    R_OOS = 1 - (df_predictions['dif_squared'].sum()/df_predictions['actual_sqaured'].sum()) \n",
    "    print(i, R_OOS)\n",
    "    total_R_2_OOS.append(R_OOS)\n",
    "\n",
    "\n",
    "np.mean(total_R_2_OOS)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.06951243422563924"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_R_2_OOS[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
