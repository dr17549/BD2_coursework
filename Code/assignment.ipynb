{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import optuna\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.regularizers import L1\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,BatchNormalization,Activation\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "panel = pd.read_pickle('../Data/returns_chars_panel.pkl') \n",
    "macro = pd.read_pickle('../Data/macro_timeseries.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>dp</th>\n",
       "      <th>ep</th>\n",
       "      <th>b/m</th>\n",
       "      <th>crsp_spvw</th>\n",
       "      <th>svar</th>\n",
       "      <th>tbl</th>\n",
       "      <th>tms</th>\n",
       "      <th>dfy</th>\n",
       "      <th>dfr</th>\n",
       "      <th>ntis</th>\n",
       "      <th>infl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>1926-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>1927-01-01</td>\n",
       "      <td>-2.973012</td>\n",
       "      <td>-2.386837</td>\n",
       "      <td>0.441476</td>\n",
       "      <td>0.026047</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>0.0307</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>-0.0022</td>\n",
       "      <td>0.050876</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>1927-02-01</td>\n",
       "      <td>-2.942374</td>\n",
       "      <td>-2.374773</td>\n",
       "      <td>0.443706</td>\n",
       "      <td>-0.002910</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>0.0323</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>0.050824</td>\n",
       "      <td>-0.011299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>1927-03-01</td>\n",
       "      <td>-2.979535</td>\n",
       "      <td>-2.430353</td>\n",
       "      <td>0.428501</td>\n",
       "      <td>0.045522</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.0329</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>0.051668</td>\n",
       "      <td>-0.005714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>1927-04-01</td>\n",
       "      <td>-2.976535</td>\n",
       "      <td>-2.445079</td>\n",
       "      <td>0.469765</td>\n",
       "      <td>0.007324</td>\n",
       "      <td>0.000924</td>\n",
       "      <td>0.0320</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>-0.0170</td>\n",
       "      <td>0.046357</td>\n",
       "      <td>-0.005747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1785</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>-3.951689</td>\n",
       "      <td>-3.108987</td>\n",
       "      <td>0.233377</td>\n",
       "      <td>0.018791</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>0.0189</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>0.0088</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>-0.010862</td>\n",
       "      <td>0.000783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786</th>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>-3.965984</td>\n",
       "      <td>-3.112869</td>\n",
       "      <td>0.232261</td>\n",
       "      <td>0.021621</td>\n",
       "      <td>0.001510</td>\n",
       "      <td>0.0165</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.0091</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>-0.013181</td>\n",
       "      <td>0.002286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1787</th>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>-3.993568</td>\n",
       "      <td>-3.130267</td>\n",
       "      <td>0.223938</td>\n",
       "      <td>0.036206</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.0154</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0088</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>-0.007820</td>\n",
       "      <td>-0.000536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1788</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>-4.015896</td>\n",
       "      <td>-3.142629</td>\n",
       "      <td>0.220116</td>\n",
       "      <td>0.029787</td>\n",
       "      <td>0.000502</td>\n",
       "      <td>0.0154</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0087</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>-0.007222</td>\n",
       "      <td>-0.000910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1789</th>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>-4.006626</td>\n",
       "      <td>-3.197893</td>\n",
       "      <td>0.222316</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>0.0152</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>-0.0113</td>\n",
       "      <td>-0.007717</td>\n",
       "      <td>0.003880</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1119 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date        dp        ep       b/m  crsp_spvw      svar     tbl  \\\n",
       "671  1926-12-01       NaN       NaN       NaN        NaN       NaN     NaN   \n",
       "672  1927-01-01 -2.973012 -2.386837  0.441476   0.026047  0.000465  0.0307   \n",
       "673  1927-02-01 -2.942374 -2.374773  0.443706  -0.002910  0.000470  0.0323   \n",
       "674  1927-03-01 -2.979535 -2.430353  0.428501   0.045522  0.000287  0.0329   \n",
       "675  1927-04-01 -2.976535 -2.445079  0.469765   0.007324  0.000924  0.0320   \n",
       "...         ...       ...       ...       ...        ...       ...     ...   \n",
       "1785 2019-10-01 -3.951689 -3.108987  0.233377   0.018791  0.000605  0.0189   \n",
       "1786 2019-11-01 -3.965984 -3.112869  0.232261   0.021621  0.001510  0.0165   \n",
       "1787 2019-12-01 -3.993568 -3.130267  0.223938   0.036206  0.000306  0.0154   \n",
       "1788 2020-01-01 -4.015896 -3.142629  0.220116   0.029787  0.000502  0.0154   \n",
       "1789 2020-02-01 -4.006626 -3.197893  0.222316   0.000108  0.001119  0.0152   \n",
       "\n",
       "         tms     dfy     dfr      ntis      infl  \n",
       "671      NaN     NaN     NaN       NaN       NaN  \n",
       "672   0.0047  0.0100 -0.0022  0.050876  0.000000  \n",
       "673   0.0028  0.0095 -0.0019  0.050824 -0.011299  \n",
       "674   0.0018  0.0092 -0.0019  0.051668 -0.005714  \n",
       "675   0.0011  0.0092 -0.0170  0.046357 -0.005747  \n",
       "...      ...     ...     ...       ...       ...  \n",
       "1785 -0.0019  0.0088  0.0002 -0.010862  0.000783  \n",
       "1786  0.0006  0.0091  0.0058 -0.013181  0.002286  \n",
       "1787  0.0027  0.0088  0.0073 -0.007820 -0.000536  \n",
       "1788  0.0032  0.0087  0.0164 -0.007222 -0.000910  \n",
       "1789  0.0024  0.0083 -0.0113 -0.007717  0.003880  \n",
       "\n",
       "[1119 rows x 12 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine micro and macro data\n",
    "df = pd.merge(panel,macro,on='date',how='left',suffixes=['','_macro']) \n",
    "\n",
    "# features + targets \n",
    "X = df.drop(columns=['ret','excess_ret','rfree','permno','date']) # everything except return info and IDs\n",
    "y = df['excess_ret'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Train Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make 20 years of training data\n",
    "date = df['date']\n",
    "training = (date <= '1977-03') # selects \n",
    "X_train, y_train = X.loc[training].values, y.loc[training].values \n",
    "\n",
    "# make 10 years of validation data\n",
    "validation = (date > '1977-03') & (date <= '1987-03') \n",
    "X_val, y_val = X.loc[validation].values, y.loc[validation].values \n",
    "\n",
    "# make test data\n",
    "test = (date > '1987-03') \n",
    "X_test, y_test = X.loc[test].values, y.loc[test].values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to create NN model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the final model \n",
    "def create_nn(n_layers, input_dim, lamda, learning_rate):\n",
    "    \n",
    "    # max nodes in first layer \n",
    "    num_layers = 32 \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "    # init model \n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_dim=input_dim, \n",
    "                kernel_regularizer=regularizers.L1(lamda), \n",
    "                kernel_initializer = 'he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # add extra hidden layers \n",
    "    for i in range(n_layers - 1): \n",
    "        num_layers = int(num_layers / 2)\n",
    "        model.add(Dense(num_layers,\n",
    "                kernel_regularizer=regularizers.L1(lamda), \n",
    "                kernel_initializer = 'he_normal'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "    # output layer \n",
    "    model.add(Dense(1, activation='linear',\n",
    "                    kernel_regularizer=regularizers.L1(lamda), \n",
    "                    kernel_initializer = 'he_normal'))\n",
    "\n",
    "    model.compile(loss='mse', \n",
    "                optimizer=optimizer,\n",
    "                metrics = ['mse']) \n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation for Lamda for L2 Regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 10:18:42,183] A new study created in memory with name: no-name-4ea5b825-f68d-477d-873a-ea743ec109b7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 10:19:06,213] Trial 0 finished with value: 0.026645848527550697 and parameters: {'learning_rate': 0.001, 'l1_reg': 0.0005436366706974076}. Best is trial 0 with value: 0.026645848527550697.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 10:19:52,434] Trial 1 finished with value: 0.02643212489783764 and parameters: {'learning_rate': 0.001, 'l1_reg': 3.274073920839725e-05}. Best is trial 1 with value: 0.02643212489783764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 10:20:18,923] Trial 2 finished with value: 0.026593651622533798 and parameters: {'learning_rate': 0.001, 'l1_reg': 0.0001586046427887883}. Best is trial 1 with value: 0.02643212489783764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 10:20:42,599] Trial 3 finished with value: 0.028688626363873482 and parameters: {'learning_rate': 0.01, 'l1_reg': 0.0005548669578422475}. Best is trial 1 with value: 0.02643212489783764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 10:21:07,792] Trial 4 finished with value: 0.027741363272070885 and parameters: {'learning_rate': 0.01, 'l1_reg': 0.0003561459577263908}. Best is trial 1 with value: 0.02643212489783764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 10:21:42,681] Trial 5 finished with value: 0.02658342570066452 and parameters: {'learning_rate': 0.001, 'l1_reg': 0.0003823760995679974}. Best is trial 1 with value: 0.02643212489783764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 10:22:31,030] Trial 6 finished with value: 0.026645097881555557 and parameters: {'learning_rate': 0.001, 'l1_reg': 7.585546665114775e-05}. Best is trial 1 with value: 0.02643212489783764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 10:23:08,125] Trial 7 finished with value: 0.026483960449695587 and parameters: {'learning_rate': 0.001, 'l1_reg': 3.6002518380953646e-05}. Best is trial 1 with value: 0.02643212489783764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 10:23:50,436] Trial 8 finished with value: 0.02637159638106823 and parameters: {'learning_rate': 0.001, 'l1_reg': 1.6762375688023723e-05}. Best is trial 8 with value: 0.02637159638106823.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 10:24:19,073] Trial 9 finished with value: 0.02843630127608776 and parameters: {'learning_rate': 0.01, 'l1_reg': 0.000497867338072132}. Best is trial 8 with value: 0.02637159638106823.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  Value: 0.02637159638106823\n",
      "  Params: \n",
      "    learning_rate: 0.001\n",
      "    l1_reg: 1.6762375688023723e-05\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters \n",
    "epochs = 100\n",
    "patience = 5\n",
    "batch_size = 10000\n",
    "\n",
    "# Using Optuna to cross validate hyper parameter \n",
    "input_dim = X_train.shape[1]\n",
    "n_layers = 4\n",
    "def create_model(trial):\n",
    "\n",
    "    num_layers = 32 \n",
    "    # Suggest hyperparameters\n",
    "    learning_rate = trial.suggest_categorical('learning_rate', [0.001, 0.01])\n",
    "    l1_reg = trial.suggest_float('l1_reg', 1e-5, 1e-3, log=True)\n",
    "\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_dim=input_dim, \n",
    "                kernel_regularizer=regularizers.L1(l1_reg), \n",
    "                kernel_initializer = 'he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # add extra hidden layers \n",
    "    for i in range(n_layers - 1): \n",
    "        num_layers = int(num_layers / 2)\n",
    "        model.add(Dense(num_layers,\n",
    "                kernel_regularizer=regularizers.L1(l1_reg), \n",
    "                kernel_initializer = 'he_normal'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "    # output layer \n",
    "    model.add(Dense(1, activation='linear',\n",
    "                    kernel_regularizer=regularizers.L1(0.01), \n",
    "                    kernel_initializer = 'he_normal'))\n",
    "\n",
    "    model.compile(loss='mse', \n",
    "                optimizer=optimizer,\n",
    "                metrics = ['mse']) \n",
    "    return model\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial):\n",
    "    model = create_model(trial)\n",
    "    \n",
    "    # Use early stopping\n",
    "    early_stopping = EarlyStopping(patience=patience, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs,\n",
    "        batch_size,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Evaluate the model\n",
    "    score = model.evaluate(X_val, y_val, verbose=0)\n",
    "    return score[0]\n",
    "\n",
    "# Create a study and optimize the objective function\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Get the best trial\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(\"Best trial:\")\n",
    "print(f\"  Value: {best_trial.value}\")\n",
    "print(\"  Params: \")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Hyperparameters \n",
    "#### NN2 \n",
    "- learning_rate = 0.001 \n",
    "- l1_reg = 1.76e-05\n",
    "\n",
    "#### NN3 \n",
    "- learning_rate = 0.001 \n",
    "- lamda = 2.91e-05\n",
    "\n",
    "#### N4 \n",
    "- learning_rate = 0.001 \n",
    "- l1_reg = 1.67e-05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expanding Window R^2_OOS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [ Code to test the R^2 OOS calculation only ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhammatornriewcharoon/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m77295/77295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 237us/step\n",
      "-0.07850356207538467\n"
     ]
    }
   ],
   "source": [
    "lamda = 1e-05\n",
    "epochs = 100\n",
    "learning_rate = 0.001\n",
    "patience = 5\n",
    "batch_size = 10000\n",
    "model = create_nn(3, X_test.shape[1], lamda , learning_rate)\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                        epochs=100, \n",
    "                        batch_size=batch_size, \n",
    "                        verbose=False,\n",
    "                        validation_data = (X_val, y_val),\n",
    "                        callbacks = [EarlyStopping(patience = patience, restore_best_weights=True)])\n",
    "predictions = model.predict(X_test)\n",
    "df_predictions = pd.DataFrame(predictions, columns=['Prediction'])\n",
    "df_predictions['Actual'] = y_test\n",
    "df_predictions['dif_squared'] = (df_predictions['Prediction'] - df_predictions['Actual'])**2\n",
    "df_predictions['actual_sqaured'] = df_predictions['Actual']**2\n",
    "R_OOS = 1 - (df_predictions['dif_squared'].sum()/df_predictions['actual_sqaured'].sum()) \n",
    "print(R_OOS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to calculate R^2 OOS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhammatornriewcharoon/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3088/3088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 231us/step\n",
      "\u001b[1m3088/3088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 237us/step\n",
      "\u001b[1m3088/3088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 227us/step\n",
      "\u001b[1m3088/3088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 226us/step\n",
      "\u001b[1m3088/3088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 227us/step\n",
      "\u001b[1m3088/3088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 229us/step\n",
      "\u001b[1m3088/3088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 226us/step\n",
      "\u001b[1m3088/3088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 229us/step\n",
      "\u001b[1m3088/3088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 235us/step\n",
      "\u001b[1m3088/3088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 229us/step\n",
      "\u001b[1m2884/2884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 236us/step\n",
      "\u001b[1m2884/2884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 244us/step\n",
      "\u001b[1m2884/2884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 233us/step\n",
      "\u001b[1m2884/2884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 231us/step\n",
      "\u001b[1m2884/2884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 243us/step\n",
      "\u001b[1m2884/2884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 232us/step\n",
      "\u001b[1m2884/2884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 227us/step\n",
      "\u001b[1m2884/2884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 233us/step\n",
      "\u001b[1m2884/2884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 232us/step\n",
      "\u001b[1m2884/2884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 280us/step\n",
      "\u001b[1m2688/2688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 261us/step\n",
      "\u001b[1m2688/2688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 223us/step\n",
      "\u001b[1m2688/2688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 225us/step\n",
      "\u001b[1m2688/2688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 223us/step\n",
      "\u001b[1m2688/2688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 238us/step\n",
      "\u001b[1m2688/2688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 225us/step\n",
      "\u001b[1m2688/2688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 226us/step\n",
      "\u001b[1m2688/2688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 224us/step\n",
      "\u001b[1m2688/2688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 229us/step\n",
      "\u001b[1m2688/2688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 225us/step\n",
      "\u001b[1m2535/2535\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 243us/step\n",
      "\u001b[1m2535/2535\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 235us/step\n",
      "\u001b[1m2535/2535\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 278us/step\n",
      "\u001b[1m2535/2535\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 286us/step\n",
      "\u001b[1m2535/2535\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 228us/step\n",
      "\u001b[1m2535/2535\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 224us/step\n",
      "\u001b[1m2535/2535\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 235us/step\n",
      "\u001b[1m2535/2535\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 228us/step\n",
      "\u001b[1m2535/2535\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 224us/step\n",
      "\u001b[1m2535/2535\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 221us/step\n",
      "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 247us/step\n",
      "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 223us/step\n",
      "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 223us/step\n",
      "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 227us/step\n",
      "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 235us/step\n",
      "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 224us/step\n",
      "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 226us/step\n",
      "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 224us/step\n",
      "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 225us/step\n",
      "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 226us/step\n",
      "\u001b[1m2510/2510\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 226us/step\n",
      "\u001b[1m2510/2510\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 236us/step\n",
      "\u001b[1m2510/2510\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 244us/step\n",
      "\u001b[1m2510/2510\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 268us/step\n",
      "\u001b[1m2510/2510\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 225us/step\n",
      "\u001b[1m2510/2510\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 242us/step\n",
      "\u001b[1m2510/2510\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 225us/step\n",
      "\u001b[1m2510/2510\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 226us/step\n",
      "\u001b[1m2510/2510\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 223us/step\n",
      "\u001b[1m2510/2510\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 241us/step\n",
      "\u001b[1m2520/2520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 248us/step\n",
      "\u001b[1m2520/2520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 226us/step\n",
      "\u001b[1m2520/2520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 228us/step\n",
      "\u001b[1m2520/2520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 228us/step\n",
      "\u001b[1m2520/2520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 228us/step\n",
      "\u001b[1m2520/2520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 226us/step\n",
      "\u001b[1m2520/2520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 240us/step\n",
      "\u001b[1m2520/2520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 231us/step\n",
      "\u001b[1m2520/2520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 227us/step\n",
      "\u001b[1m2520/2520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 242us/step\n",
      "\u001b[1m2554/2554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 227us/step\n",
      "\u001b[1m2554/2554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 227us/step\n",
      "\u001b[1m2554/2554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 224us/step\n",
      "\u001b[1m2554/2554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 243us/step\n",
      "\u001b[1m2554/2554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 229us/step\n",
      "\u001b[1m2554/2554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 227us/step\n",
      "\u001b[1m2554/2554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 245us/step\n",
      "\u001b[1m2554/2554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 230us/step\n",
      "\u001b[1m2554/2554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 229us/step\n",
      "\u001b[1m2554/2554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 228us/step\n",
      "\u001b[1m2451/2451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 229us/step\n",
      "\u001b[1m2451/2451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 229us/step\n",
      "\u001b[1m2451/2451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 227us/step\n",
      "\u001b[1m2451/2451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 241us/step\n",
      "\u001b[1m2451/2451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 230us/step\n",
      "\u001b[1m2451/2451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 228us/step\n",
      "\u001b[1m2451/2451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 230us/step\n",
      "\u001b[1m2451/2451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 240us/step\n",
      "\u001b[1m2451/2451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 229us/step\n",
      "\u001b[1m2451/2451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 241us/step\n",
      "\u001b[1m2195/2195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 257us/step\n",
      "\u001b[1m2195/2195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 231us/step\n",
      "\u001b[1m2195/2195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 235us/step\n",
      "\u001b[1m2195/2195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 230us/step\n",
      "\u001b[1m2195/2195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 236us/step\n",
      "\u001b[1m2195/2195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 229us/step\n",
      "\u001b[1m2195/2195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 226us/step\n",
      "\u001b[1m2195/2195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 240us/step\n",
      "\u001b[1m2195/2195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218us/step\n",
      "\u001b[1m2195/2195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 228us/step\n",
      "\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 229us/step\n",
      "\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 234us/step\n",
      "\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 227us/step\n",
      "\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 224us/step\n",
      "\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 228us/step\n",
      "\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 228us/step\n",
      "\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 230us/step\n",
      "\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 235us/step\n",
      "\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 238us/step\n",
      "\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 228us/step\n",
      "\u001b[1m2105/2105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 231us/step\n",
      "\u001b[1m2105/2105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 283us/step\n",
      "\u001b[1m2105/2105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227us/step\n",
      "\u001b[1m2105/2105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 284us/step\n",
      "\u001b[1m2105/2105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220us/step\n",
      "\u001b[1m2105/2105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 407us/step\n",
      "\u001b[1m2105/2105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 245us/step\n",
      "\u001b[1m2105/2105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226us/step\n",
      "\u001b[1m2105/2105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 231us/step\n",
      "\u001b[1m2105/2105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 236us/step\n",
      "\u001b[1m2073/2073\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229us/step\n",
      "\u001b[1m2073/2073\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227us/step\n",
      "\u001b[1m2073/2073\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 236us/step\n",
      "\u001b[1m2073/2073\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226us/step\n",
      "\u001b[1m2073/2073\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225us/step\n",
      "\u001b[1m2073/2073\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226us/step\n",
      "\u001b[1m2073/2073\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 232us/step\n",
      "\u001b[1m2073/2073\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225us/step\n",
      "\u001b[1m2073/2073\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229us/step\n",
      "\u001b[1m2073/2073\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219us/step\n",
      "\u001b[1m2056/2056\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 463us/step\n",
      "\u001b[1m2056/2056\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224us/step\n",
      "\u001b[1m2056/2056\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 258us/step\n",
      "\u001b[1m2056/2056\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 256us/step\n",
      "\u001b[1m2056/2056\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222us/step\n",
      "\u001b[1m2056/2056\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 239us/step\n",
      "\u001b[1m2056/2056\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227us/step\n",
      "\u001b[1m2056/2056\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224us/step\n",
      "\u001b[1m2056/2056\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227us/step\n",
      "\u001b[1m2056/2056\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230us/step\n",
      "\u001b[1m2115/2115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 234us/step\n",
      "\u001b[1m2115/2115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 229us/step\n",
      "\u001b[1m2115/2115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226us/step\n",
      "\u001b[1m2115/2115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 226us/step\n",
      "\u001b[1m2115/2115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 241us/step\n",
      "\u001b[1m2115/2115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226us/step\n",
      "\u001b[1m2115/2115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 228us/step\n",
      "\u001b[1m2115/2115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 247us/step\n",
      "\u001b[1m2115/2115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 226us/step\n",
      "\u001b[1m2115/2115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225us/step\n",
      "Final R^2 OOS :   -0.15262246169708324\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "end_year = date.max()\n",
    "\n",
    "# Hyperparameters \n",
    "lamda = 1e-05\n",
    "epochs = 100\n",
    "learning_rate = 0.001\n",
    "patience = 5\n",
    "batch_size = 10000\n",
    "ensemble = 10 \n",
    "hidden_layers = 3\n",
    "\n",
    "# Size of rolling window in years \n",
    "val_length = 12 \n",
    "test_length = 1\n",
    "\n",
    "total_R_2_OOS = [] \n",
    "model = create_nn(hidden_layers, X.shape[1], lamda , learning_rate)\n",
    "\n",
    "for i in range(1988, end_year.year - val_length - test_length):\n",
    "    print(i)\n",
    "    predictions = [] \n",
    "    training_window = (date < datetime(i,1,1)) \n",
    "    validation_window = (date >= datetime(i,1,1)) & (date < datetime(i+val_length,1,1)) \n",
    "    test_window = (date >= datetime(i+val_length,1,1)) & (date < datetime(i+val_length+test_length,1,1))\n",
    "\n",
    "    X_train_expanding, y_train_expanding = X.loc[training_window].values, y.loc[training_window].values\n",
    "    X_val_expanding , y_val_expanding = X.loc[validation_window].values, y.loc[validation_window].values\n",
    "    X_test_expanding, y_test_expanding =  X.loc[test_window].values, y.loc[test_window].values\n",
    "\n",
    "    # take the final output as the average of ensemble number of NN \n",
    "    for j in range(ensemble): \n",
    "        # set random seed each time the model is trained \n",
    "        seed = int(time.time())\n",
    "        tf.random.set_seed(seed)\n",
    "        history = model.fit(X_train_expanding, y_train_expanding, \n",
    "                            epochs=epochs, \n",
    "                            batch_size=batch_size, \n",
    "                            verbose=False,\n",
    "                            validation_data = (X_val_expanding, y_val_expanding),\n",
    "                            callbacks = [EarlyStopping(patience = patience, restore_best_weights=True)])\n",
    "        current_prediction = model.predict(X_test_expanding)\n",
    "        # Average the predictions 10 times from 10 different nueral network models \n",
    "        if len(predictions) == 0:\n",
    "            predictions = current_prediction\n",
    "        else: \n",
    "            predictions = (predictions + current_prediction) /2 \n",
    "\n",
    "    df_predictions = pd.DataFrame(predictions, columns=['Prediction'])\n",
    "    df_predictions['Actual'] = y_test_expanding\n",
    "    df_predictions['dif_squared'] = (df_predictions['Prediction'] - df_predictions['Actual'])**2\n",
    "    df_predictions['actual_sqaured'] = df_predictions['Actual']**2\n",
    "    R_OOS = 1 - (df_predictions['dif_squared'].sum()/df_predictions['actual_sqaured'].sum()) \n",
    "    # print(\"***** R^2_OOS \", i, R_OOS)\n",
    "    total_R_2_OOS.append(R_OOS)\n",
    "    \n",
    "# calculate the mean OOS for all time periods\n",
    "print(\"Final R^2 OOS :  \", np.mean(total_R_2_OOS)) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
