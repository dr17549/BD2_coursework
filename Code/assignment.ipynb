{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import optuna\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.regularizers import L1\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,BatchNormalization,Activation\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "panel = pd.read_pickle('../Data/returns_chars_panel.pkl') \n",
    "macro = pd.read_pickle('../Data/macro_timeseries.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine micro and macro data\n",
    "df = pd.merge(panel,macro,on='date',how='left',suffixes=['','_macro']) \n",
    "\n",
    "# features + targets \n",
    "X = df.drop(columns=['ret','excess_ret','rfree','permno','date']) # everything except return info and IDs\n",
    "y = df['excess_ret'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Train Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make 20 years of training data\n",
    "date = df['date']\n",
    "training = (date <= '1977-03') # selects \n",
    "X_train, y_train = X.loc[training].values, y.loc[training].values \n",
    "\n",
    "# make 10 years of validation data\n",
    "validation = (date > '1977-03') & (date <= '1987-03') \n",
    "X_val, y_val = X.loc[validation].values, y.loc[validation].values \n",
    "\n",
    "# make test data\n",
    "test = (date > '1987-03') \n",
    "X_test, y_test = X.loc[test].values, y.loc[test].values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to create NN model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the final model \n",
    "def create_nn(n_layers, input_dim, lamda, learning_rate):\n",
    "    \n",
    "    # max nodes in first layer \n",
    "    num_layers = 32 \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "    # init model \n",
    "    model = Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=(input_dim,)))\n",
    "    model.add(Dense(32,\n",
    "                kernel_regularizer=regularizers.L1(lamda), \n",
    "                kernel_initializer = 'he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # add extra hidden layers \n",
    "    for i in range(n_layers - 1): \n",
    "        num_layers = int(num_layers / 2)\n",
    "        model.add(Dense(num_layers,\n",
    "                kernel_regularizer=regularizers.L1(lamda), \n",
    "                kernel_initializer = 'he_normal'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "    # output layer \n",
    "    model.add(Dense(1, activation='linear',\n",
    "                    kernel_regularizer=regularizers.L1(lamda), \n",
    "                    kernel_initializer = 'he_normal'))\n",
    "\n",
    "    model.compile(loss='mse', \n",
    "                optimizer=optimizer,\n",
    "                metrics = ['mse']) \n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation for Lamda for L2 Regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 10:18:42,183] A new study created in memory with name: no-name-4ea5b825-f68d-477d-873a-ea743ec109b7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 10:19:06,213] Trial 0 finished with value: 0.026645848527550697 and parameters: {'learning_rate': 0.001, 'l1_reg': 0.0005436366706974076}. Best is trial 0 with value: 0.026645848527550697.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 10:19:52,434] Trial 1 finished with value: 0.02643212489783764 and parameters: {'learning_rate': 0.001, 'l1_reg': 3.274073920839725e-05}. Best is trial 1 with value: 0.02643212489783764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 10:20:18,923] Trial 2 finished with value: 0.026593651622533798 and parameters: {'learning_rate': 0.001, 'l1_reg': 0.0001586046427887883}. Best is trial 1 with value: 0.02643212489783764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 10:20:42,599] Trial 3 finished with value: 0.028688626363873482 and parameters: {'learning_rate': 0.01, 'l1_reg': 0.0005548669578422475}. Best is trial 1 with value: 0.02643212489783764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 10:21:07,792] Trial 4 finished with value: 0.027741363272070885 and parameters: {'learning_rate': 0.01, 'l1_reg': 0.0003561459577263908}. Best is trial 1 with value: 0.02643212489783764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 10:21:42,681] Trial 5 finished with value: 0.02658342570066452 and parameters: {'learning_rate': 0.001, 'l1_reg': 0.0003823760995679974}. Best is trial 1 with value: 0.02643212489783764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 10:22:31,030] Trial 6 finished with value: 0.026645097881555557 and parameters: {'learning_rate': 0.001, 'l1_reg': 7.585546665114775e-05}. Best is trial 1 with value: 0.02643212489783764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 10:23:08,125] Trial 7 finished with value: 0.026483960449695587 and parameters: {'learning_rate': 0.001, 'l1_reg': 3.6002518380953646e-05}. Best is trial 1 with value: 0.02643212489783764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 10:23:50,436] Trial 8 finished with value: 0.02637159638106823 and parameters: {'learning_rate': 0.001, 'l1_reg': 1.6762375688023723e-05}. Best is trial 8 with value: 0.02637159638106823.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 10:24:19,073] Trial 9 finished with value: 0.02843630127608776 and parameters: {'learning_rate': 0.01, 'l1_reg': 0.000497867338072132}. Best is trial 8 with value: 0.02637159638106823.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  Value: 0.02637159638106823\n",
      "  Params: \n",
      "    learning_rate: 0.001\n",
      "    l1_reg: 1.6762375688023723e-05\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters \n",
    "epochs = 100\n",
    "patience = 5\n",
    "batch_size = 10000\n",
    "\n",
    "# Using Optuna to cross validate hyper parameter \n",
    "input_dim = X_train.shape[1]\n",
    "n_layers = 4\n",
    "def create_model(trial):\n",
    "\n",
    "    num_layers = 32 \n",
    "    # Suggest hyperparameters\n",
    "    learning_rate = trial.suggest_categorical('learning_rate', [0.001, 0.01])\n",
    "    l1_reg = trial.suggest_float('l1_reg', 1e-5, 1e-3, log=True)\n",
    "\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_dim=input_dim, \n",
    "                kernel_regularizer=regularizers.L1(l1_reg), \n",
    "                kernel_initializer = 'he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # add extra hidden layers \n",
    "    for i in range(n_layers - 1): \n",
    "        num_layers = int(num_layers / 2)\n",
    "        model.add(Dense(num_layers,\n",
    "                kernel_regularizer=regularizers.L1(l1_reg), \n",
    "                kernel_initializer = 'he_normal'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "    # output layer \n",
    "    model.add(Dense(1, activation='linear',\n",
    "                    kernel_regularizer=regularizers.L1(0.01), \n",
    "                    kernel_initializer = 'he_normal'))\n",
    "\n",
    "    model.compile(loss='mse', \n",
    "                optimizer=optimizer,\n",
    "                metrics = ['mse']) \n",
    "    return model\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial):\n",
    "    model = create_model(trial)\n",
    "    \n",
    "    # Use early stopping\n",
    "    early_stopping = EarlyStopping(patience=patience, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs,\n",
    "        batch_size,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Evaluate the model\n",
    "    score = model.evaluate(X_val, y_val, verbose=0)\n",
    "    return score[0]\n",
    "\n",
    "# Create a study and optimize the objective function\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Get the best trial\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(\"Best trial:\")\n",
    "print(f\"  Value: {best_trial.value}\")\n",
    "print(\"  Params: \")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Hyperparameters \n",
    "#### NN2 \n",
    "- learning_rate = 0.001 \n",
    "- l1_reg = 1.76e-05\n",
    "\n",
    "#### NN3 \n",
    "- learning_rate = 0.001 \n",
    "- lamda = 2.91e-05\n",
    "\n",
    "#### N4 \n",
    "- learning_rate = 0.001 \n",
    "- l1_reg = 1.67e-05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expanding Window R^2_OOS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [ Code to test the R^2 OOS calculation only ]\n",
    "## Standardized Version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhammatornriewcharoon/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m77295/77295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 274us/step\n",
      "-0.31440565877466087\n"
     ]
    }
   ],
   "source": [
    "lamda = 1e-05\n",
    "epochs = 100\n",
    "learning_rate = 0.001\n",
    "patience = 5\n",
    "batch_size = 10000\n",
    "model = create_nn(3, X_test.shape[1], lamda , learning_rate)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_expanding = scaler.fit_transform(X_train)\n",
    "X_val_expanding = scaler.transform(X_val)\n",
    "X_test_expanding = scaler.transform(X_test)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_expanding, y_train)).batch(32)\n",
    "validate_dataset = tf.data.Dataset.from_tensor_slices((X_val_expanding, y_val)).batch(32)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(32)\n",
    "\n",
    "history = model.fit(train_dataset, \n",
    "                        epochs=100, \n",
    "                        batch_size=32, \n",
    "                        verbose=False,\n",
    "                        validation_data = validate_dataset,\n",
    "                        callbacks = [EarlyStopping(patience = patience, restore_best_weights=True)])\n",
    "predictions = model.predict(X_test)\n",
    "df_predictions = pd.DataFrame(predictions, columns=['Prediction'])\n",
    "df_predictions['Actual'] = y_test\n",
    "df_predictions['dif_squared'] = (df_predictions['Prediction'] - df_predictions['Actual'])**2\n",
    "df_predictions['actual_sqaured'] = df_predictions['Actual']**2\n",
    "R_OOS = 1 - (df_predictions['dif_squared'].sum()/df_predictions['actual_sqaured'].sum()) \n",
    "print(R_OOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m77295/77295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 247us/step\n",
      "-0.053041720788316704\n"
     ]
    }
   ],
   "source": [
    "lamda = 1e-05\n",
    "epochs = 100\n",
    "learning_rate = 0.001\n",
    "patience = 5\n",
    "batch_size = 10000\n",
    "model = create_nn(3, X_test.shape[1], lamda , learning_rate)\n",
    "\n",
    "history = model.fit(X_train_expanding, y_train, \n",
    "                        epochs=100, \n",
    "                        batch_size=batch_size, \n",
    "                        verbose=False,\n",
    "                        validation_data = (X_val_expanding, y_val),\n",
    "                        callbacks = [EarlyStopping(patience = patience, restore_best_weights=True)])\n",
    "predictions = model.predict(X_test)\n",
    "df_predictions = pd.DataFrame(predictions, columns=['Prediction'])\n",
    "df_predictions['Actual'] = y_test\n",
    "df_predictions['dif_squared'] = (df_predictions['Prediction'] - df_predictions['Actual'])**2\n",
    "df_predictions['actual_sqaured'] = df_predictions['Actual']**2\n",
    "R_OOS = 1 - (df_predictions['dif_squared'].sum()/df_predictions['actual_sqaured'].sum()) \n",
    "print(R_OOS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to calculate R^2 OOS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhammatornriewcharoon/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1988\n",
      "ensemble # 0\n",
      "predicting ...\n",
      "\u001b[1m3088/3088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 213us/step\n",
      "ensemble # 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime())\n\u001b[1;32m     46\u001b[0m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mset_seed(seed)\n\u001b[0;32m---> 47\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train_expanding, y_train_expanding, \n\u001b[1;32m     48\u001b[0m                     epochs\u001b[38;5;241m=\u001b[39mepochs, \n\u001b[1;32m     49\u001b[0m                     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, \n\u001b[1;32m     50\u001b[0m                     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     51\u001b[0m                     validation_data \u001b[38;5;241m=\u001b[39m (X_val_expanding, y_val_expanding),\n\u001b[1;32m     52\u001b[0m                     callbacks \u001b[38;5;241m=\u001b[39m [EarlyStopping(patience \u001b[38;5;241m=\u001b[39m patience, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)])\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicting ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     54\u001b[0m current_prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test_expanding)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:339\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_epoch_iterator\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_epoch_iterator \u001b[38;5;241m=\u001b[39m TFEpochIterator(\n\u001b[1;32m    330\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[1;32m    331\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    337\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    338\u001b[0m     )\n\u001b[0;32m--> 339\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(\n\u001b[1;32m    340\u001b[0m     x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[1;32m    341\u001b[0m     y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[1;32m    342\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39mval_sample_weight,\n\u001b[1;32m    343\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mvalidation_batch_size \u001b[38;5;129;01mor\u001b[39;00m batch_size,\n\u001b[1;32m    344\u001b[0m     steps\u001b[38;5;241m=\u001b[39mvalidation_steps,\n\u001b[1;32m    345\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    346\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    347\u001b[0m     _use_cached_eval_dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    348\u001b[0m )\n\u001b[1;32m    349\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    351\u001b[0m }\n\u001b[1;32m    352\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:425\u001b[0m, in \u001b[0;36mTensorFlowTrainer.evaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    424\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[0;32m--> 425\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_function(iterator)\n\u001b[1;32m    426\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    427\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_end(step, logs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m    879\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[1;32m    880\u001b[0m )\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:132\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    130\u001b[0m args \u001b[38;5;241m=\u001b[39m args \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;28;01melse\u001b[39;00m ()\n\u001b[1;32m    131\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m--> 132\u001b[0m function \u001b[38;5;241m=\u001b[39m trace_function(\n\u001b[1;32m    133\u001b[0m     args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs, tracing_options\u001b[38;5;241m=\u001b[39mtracing_options\n\u001b[1;32m    134\u001b[0m )\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Bind it ourselves to skip unnecessary canonicalization of default call.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:178\u001b[0m, in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    175\u001b[0m     args \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39minput_signature\n\u001b[1;32m    176\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 178\u001b[0m   concrete_function \u001b[38;5;241m=\u001b[39m _maybe_define_function(\n\u001b[1;32m    179\u001b[0m       args, kwargs, tracing_options\n\u001b[1;32m    180\u001b[0m   )\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mbind_graph_to_function:\n\u001b[1;32m    183\u001b[0m   concrete_function\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:230\u001b[0m, in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    220\u001b[0m current_func_context \u001b[38;5;241m=\u001b[39m function_context\u001b[38;5;241m.\u001b[39mmake_function_context(\n\u001b[1;32m    221\u001b[0m     tracing_options\u001b[38;5;241m.\u001b[39mscope_type\n\u001b[1;32m    222\u001b[0m )\n\u001b[1;32m    224\u001b[0m capture_types \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    225\u001b[0m     tracing_options\u001b[38;5;241m.\u001b[39mfunction_captures\u001b[38;5;241m.\u001b[39mcapture_types\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mfunction_captures\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    228\u001b[0m )\n\u001b[1;32m    229\u001b[0m lookup_func_type, lookup_func_context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 230\u001b[0m     function_type_utils\u001b[38;5;241m.\u001b[39mmake_canonicalized_monomorphic_type(\n\u001b[1;32m    231\u001b[0m         args,\n\u001b[1;32m    232\u001b[0m         kwargs,\n\u001b[1;32m    233\u001b[0m         capture_types,\n\u001b[1;32m    234\u001b[0m         tracing_options\u001b[38;5;241m.\u001b[39mpolymorphic_type,\n\u001b[1;32m    235\u001b[0m     )\n\u001b[1;32m    236\u001b[0m )\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    239\u001b[0m   concrete_function \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache\u001b[38;5;241m.\u001b[39mlookup(\n\u001b[1;32m    240\u001b[0m       lookup_func_type, current_func_context\n\u001b[1;32m    241\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/function_type_utils.py:375\u001b[0m, in \u001b[0;36mmake_canonicalized_monomorphic_type\u001b[0;34m(args, kwargs, capture_types, polymorphic_type)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generates function type given the function arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m    369\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    370\u001b[0m     function_type_lib\u001b[38;5;241m.\u001b[39msanitize_arg_name(name): value\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    372\u001b[0m }\n\u001b[1;32m    374\u001b[0m function_type, type_context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 375\u001b[0m     function_type_lib\u001b[38;5;241m.\u001b[39mcanonicalize_to_monomorphic(\n\u001b[1;32m    376\u001b[0m         args, kwargs, {}, capture_types, polymorphic_type\n\u001b[1;32m    377\u001b[0m     )\n\u001b[1;32m    378\u001b[0m )\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function_type, type_context\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/core/function/polymorphism/function_type.py:583\u001b[0m, in \u001b[0;36mcanonicalize_to_monomorphic\u001b[0;34m(args, kwargs, default_values, capture_types, polymorphic_type)\u001b[0m\n\u001b[1;32m    577\u001b[0m       parameters\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    578\u001b[0m           _make_validated_mono_param(kwarg_name, arg[kwarg_name],\n\u001b[1;32m    579\u001b[0m                                      Parameter\u001b[38;5;241m.\u001b[39mKEYWORD_ONLY, type_context,\n\u001b[1;32m    580\u001b[0m                                      poly_parameter\u001b[38;5;241m.\u001b[39mtype_constraint))\n\u001b[1;32m    581\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    582\u001b[0m     parameters\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 583\u001b[0m         _make_validated_mono_param(name, arg, poly_parameter\u001b[38;5;241m.\u001b[39mkind,\n\u001b[1;32m    584\u001b[0m                                    type_context,\n\u001b[1;32m    585\u001b[0m                                    poly_parameter\u001b[38;5;241m.\u001b[39mtype_constraint))\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m FunctionType(parameters, capture_types), type_context\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/core/function/polymorphism/function_type.py:522\u001b[0m, in \u001b[0;36m_make_validated_mono_param\u001b[0;34m(name, value, kind, type_context, poly_type)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_validated_mono_param\u001b[39m(\n\u001b[1;32m    519\u001b[0m     name, value, kind, type_context, poly_type\n\u001b[1;32m    520\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Parameter:\n\u001b[1;32m    521\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Generates and validates a parameter for Monomorphic FunctionType.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 522\u001b[0m   mono_type \u001b[38;5;241m=\u001b[39m trace_type\u001b[38;5;241m.\u001b[39mfrom_value(value, type_context)\n\u001b[1;32m    524\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m poly_type \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mono_type\u001b[38;5;241m.\u001b[39mis_subtype_of(poly_type):\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` was expected to be of type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    526\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpoly_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmono_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/core/function/trace_type/trace_type_builder.py:145\u001b[0m, in \u001b[0;36mfrom_value\u001b[0;34m(value, context)\u001b[0m\n\u001b[1;32m    143\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, trace\u001b[38;5;241m.\u001b[39mSupportsTracingProtocol):\n\u001b[0;32m--> 145\u001b[0m   generated_type \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39m__tf_tracing_type__(context)\n\u001b[1;32m    146\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(generated_type, trace\u001b[38;5;241m.\u001b[39mTraceType):\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected an instance of TraceType for Tracing Protocol call to \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;28mstr\u001b[39m(value) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(generated_type))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py:909\u001b[0m, in \u001b[0;36mOwnedIterator.__tf_tracing_type__\u001b[0;34m(self, _)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__tf_tracing_type__\u001b[39m(\u001b[38;5;28mself\u001b[39m, _):\n\u001b[0;32m--> 909\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_type_spec\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py:805\u001b[0m, in \u001b[0;36mOwnedIterator._type_spec\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_type_spec\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 805\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m IteratorSpec(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39melement_spec)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py:857\u001b[0m, in \u001b[0;36mOwnedIterator.element_spec\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the type of each component of an element of this iterator.\u001b[39;00m\n\u001b[1;32m    848\u001b[0m \n\u001b[1;32m    849\u001b[0m \u001b[38;5;124;03m  Returns:\u001b[39;00m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;124;03m    A (nested) structure of `tf.DType` objects corresponding to each component\u001b[39;00m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;124;03m    of an element of this dataset.\u001b[39;00m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m    853\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m    854\u001b[0m       \u001b[38;5;28;01mlambda\u001b[39;00m component_spec: component_spec\u001b[38;5;241m.\u001b[39m_to_legacy_output_types(),  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    855\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec)\n\u001b[0;32m--> 857\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21melement_spec\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    859\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_next\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "end_year = date.max()\n",
    "\n",
    "# Hyperparameters \n",
    "lamda = 1e-05\n",
    "epochs = 100\n",
    "learning_rate = 0.001\n",
    "patience = 5\n",
    "batch_size = 10000\n",
    "ensemble = 10 \n",
    "hidden_layers = 3\n",
    "\n",
    "# Size of rolling window in years \n",
    "val_length = 12 \n",
    "test_length = 1\n",
    "\n",
    "total_R_2_OOS = [] \n",
    "model = create_nn(hidden_layers, X.shape[1], lamda , learning_rate)\n",
    "\n",
    "for i in range(1988, end_year.year - val_length - test_length):\n",
    "    print(i)\n",
    "    predictions = [] \n",
    "    training_window = (date < datetime(i,1,1)) \n",
    "    validation_window = (date >= datetime(i,1,1)) & (date < datetime(i+val_length,1,1)) \n",
    "    test_window = (date >= datetime(i+val_length,1,1)) & (date < datetime(i+val_length+test_length,1,1))\n",
    "\n",
    "    X_train_expanding, y_train_expanding = X.loc[training_window].values, y.loc[training_window].values\n",
    "    X_val_expanding , y_val_expanding = X.loc[validation_window].values, y.loc[validation_window].values\n",
    "    X_test_expanding, y_test_expanding =  X.loc[test_window].values, y.loc[test_window].values\n",
    "\n",
    "    # # # standardized features \n",
    "    # scaler = StandardScaler()\n",
    "    # X_train_expanding = scaler.fit_transform(X_train_expanding)\n",
    "    # X_val_expanding = scaler.transform(X_val_expanding)\n",
    "    # X_test_expanding = scaler.transform(X_test_expanding)\n",
    "\n",
    "    # train_dataset = tf.data.Dataset.from_tensor_slices((X_train_expanding, y_train_expanding)).batch(32)\n",
    "    # validate_dataset = tf.data.Dataset.from_tensor_slices((X_val_expanding, y_val_expanding)).batch(32)\n",
    "    # test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(32)\n",
    "\n",
    "    # take the final output as the average of ensemble number of NN \n",
    "    for j in range(ensemble): \n",
    "        print(\"ensemble #\" , j)\n",
    "        # set random seed each time the model is trained \n",
    "        seed = int(time.time())\n",
    "        tf.random.set_seed(seed)\n",
    "        history = model.fit(train_dataset, \n",
    "                            epochs=epochs, \n",
    "                            batch_size=32, \n",
    "                            verbose=False,\n",
    "                            validation_data = validate_dataset,\n",
    "                            callbacks = [EarlyStopping(patience = patience, restore_best_weights=True)])\n",
    "        print(\"predicting ...\")\n",
    "        current_prediction = model.predict(X_test_expanding)\n",
    "        # Average the predictions 10 times from 10 different nueral network models \n",
    "        if len(predictions) == 0:\n",
    "            predictions = current_prediction\n",
    "        else: \n",
    "            predictions = (predictions + current_prediction)\n",
    "\n",
    "    predictions = predictions / 10 \n",
    "    df_predictions = pd.DataFrame(predictions, columns=['Prediction'])\n",
    "    df_predictions['Actual'] = y_test_expanding\n",
    "    df_predictions['dif_squared'] = (df_predictions['Prediction'] - df_predictions['Actual'])**2\n",
    "    df_predictions['actual_sqaured'] = df_predictions['Actual']**2\n",
    "    R_OOS = 1 - (df_predictions['dif_squared'].sum()/df_predictions['actual_sqaured'].sum()) \n",
    "    print(\"***** R^2_OOS \", i, R_OOS)\n",
    "    total_R_2_OOS.append(R_OOS)\n",
    "    \n",
    "# calculate the mean OOS for all time periods\n",
    "print(\"Final R^2 OOS :  \", np.mean(total_R_2_OOS)) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
