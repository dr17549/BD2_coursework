{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import optuna\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.regularizers import L1\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,BatchNormalization,Activation\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "\n",
    "panel = pd.read_pickle('../Data/returns_chars_panel.pkl') \n",
    "macro = pd.read_pickle('../Data/macro_timeseries.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine micro and macro data\n",
    "df = pd.merge(panel,macro,on='date',how='left',suffixes=['','_macro']) \n",
    "\n",
    "# features + targets \n",
    "X = df.drop(columns=['ret','excess_ret','rfree','permno','date']) # everything except return info and IDs\n",
    "y = df['excess_ret'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Train Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make 20 years of training data\n",
    "date = df['date']\n",
    "training = (date <= '1977-03') # selects \n",
    "X_train, y_train = X.loc[training].values, y.loc[training].values \n",
    "\n",
    "# make 10 years of validation data\n",
    "validation = (date > '1977-03') & (date <= '1987-03') \n",
    "X_val, y_val = X.loc[validation].values, y.loc[validation].values \n",
    "\n",
    "# make test data\n",
    "test = (date > '1987-03') \n",
    "X_test, y_test = X.loc[test].values, y.loc[test].values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to create NN model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the final model \n",
    "def create_nn(n_layers, input_dim, lamda, learning_rate):\n",
    "    \n",
    "    # max nodes in first layer \n",
    "    num_layers = 32 \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "    # init model \n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_dim=input_dim, \n",
    "                kernel_regularizer=regularizers.L1(lamda), \n",
    "                kernel_initializer = 'he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # add extra hidden layers \n",
    "    for i in range(n_layers - 1): \n",
    "        num_layers = int(num_layers / 2)\n",
    "        model.add(Dense(num_layers,\n",
    "                kernel_regularizer=regularizers.L1(lamda), \n",
    "                kernel_initializer = 'he_normal'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "    # output layer \n",
    "    model.add(Dense(1, activation='linear',\n",
    "                    kernel_regularizer=regularizers.L1(lamda), \n",
    "                    kernel_initializer = 'he_normal'))\n",
    "\n",
    "    model.compile(loss='mse', \n",
    "                optimizer=optimizer,\n",
    "                metrics = ['mse']) \n",
    "    return model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation for Lamda for L2 Regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 10:18:42,183] A new study created in memory with name: no-name-4ea5b825-f68d-477d-873a-ea743ec109b7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 10:19:06,213] Trial 0 finished with value: 0.026645848527550697 and parameters: {'learning_rate': 0.001, 'l1_reg': 0.0005436366706974076}. Best is trial 0 with value: 0.026645848527550697.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 10:19:52,434] Trial 1 finished with value: 0.02643212489783764 and parameters: {'learning_rate': 0.001, 'l1_reg': 3.274073920839725e-05}. Best is trial 1 with value: 0.02643212489783764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 10:20:18,923] Trial 2 finished with value: 0.026593651622533798 and parameters: {'learning_rate': 0.001, 'l1_reg': 0.0001586046427887883}. Best is trial 1 with value: 0.02643212489783764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 10:20:42,599] Trial 3 finished with value: 0.028688626363873482 and parameters: {'learning_rate': 0.01, 'l1_reg': 0.0005548669578422475}. Best is trial 1 with value: 0.02643212489783764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 10:21:07,792] Trial 4 finished with value: 0.027741363272070885 and parameters: {'learning_rate': 0.01, 'l1_reg': 0.0003561459577263908}. Best is trial 1 with value: 0.02643212489783764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 10:21:42,681] Trial 5 finished with value: 0.02658342570066452 and parameters: {'learning_rate': 0.001, 'l1_reg': 0.0003823760995679974}. Best is trial 1 with value: 0.02643212489783764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 10:22:31,030] Trial 6 finished with value: 0.026645097881555557 and parameters: {'learning_rate': 0.001, 'l1_reg': 7.585546665114775e-05}. Best is trial 1 with value: 0.02643212489783764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 10:23:08,125] Trial 7 finished with value: 0.026483960449695587 and parameters: {'learning_rate': 0.001, 'l1_reg': 3.6002518380953646e-05}. Best is trial 1 with value: 0.02643212489783764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 10:23:50,436] Trial 8 finished with value: 0.02637159638106823 and parameters: {'learning_rate': 0.001, 'l1_reg': 1.6762375688023723e-05}. Best is trial 8 with value: 0.02637159638106823.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 10:24:19,073] Trial 9 finished with value: 0.02843630127608776 and parameters: {'learning_rate': 0.01, 'l1_reg': 0.000497867338072132}. Best is trial 8 with value: 0.02637159638106823.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  Value: 0.02637159638106823\n",
      "  Params: \n",
      "    learning_rate: 0.001\n",
      "    l1_reg: 1.6762375688023723e-05\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters \n",
    "epochs = 100\n",
    "patience = 5\n",
    "batch_size = 10000\n",
    "\n",
    "# Using Optuna to cross validate hyper parameter \n",
    "input_dim = X_train.shape[1]\n",
    "n_layers = 4\n",
    "def create_model(trial):\n",
    "\n",
    "    num_layers = 32 \n",
    "    # Suggest hyperparameters\n",
    "    learning_rate = trial.suggest_categorical('learning_rate', [0.001, 0.01])\n",
    "    l1_reg = trial.suggest_float('l1_reg', 1e-5, 1e-3, log=True)\n",
    "\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_dim=input_dim, \n",
    "                kernel_regularizer=regularizers.L1(l1_reg), \n",
    "                kernel_initializer = 'he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # add extra hidden layers \n",
    "    for i in range(n_layers - 1): \n",
    "        num_layers = int(num_layers / 2)\n",
    "        model.add(Dense(num_layers,\n",
    "                kernel_regularizer=regularizers.L1(l1_reg), \n",
    "                kernel_initializer = 'he_normal'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "    # output layer \n",
    "    model.add(Dense(1, activation='linear',\n",
    "                    kernel_regularizer=regularizers.L1(0.01), \n",
    "                    kernel_initializer = 'he_normal'))\n",
    "\n",
    "    model.compile(loss='mse', \n",
    "                optimizer=optimizer,\n",
    "                metrics = ['mse']) \n",
    "    return model\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial):\n",
    "    model = create_model(trial)\n",
    "    \n",
    "    # Use early stopping\n",
    "    early_stopping = EarlyStopping(patience=patience, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs,\n",
    "        batch_size,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Evaluate the model\n",
    "    score = model.evaluate(X_val, y_val, verbose=0)\n",
    "    return score[0]\n",
    "\n",
    "# Create a study and optimize the objective function\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Get the best trial\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(\"Best trial:\")\n",
    "print(f\"  Value: {best_trial.value}\")\n",
    "print(\"  Params: \")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Hyperparameters \n",
    "#### NN2 \n",
    "- learning_rate = 0.001 \n",
    "- l1_reg = 1.76e-05\n",
    "\n",
    "#### NN3 \n",
    "- learning_rate = 0.001 \n",
    "- lamda = 2.91e-05\n",
    "\n",
    "#### N4 \n",
    "- learning_rate = 0.001 \n",
    "- l1_reg = 1.67e-05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expanding Window R^2_OOS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [ Code to test the R^2 OOS calculation only ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhammatornriewcharoon/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m77295/77295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 237us/step\n",
      "-0.07850356207538467\n"
     ]
    }
   ],
   "source": [
    "lamda = 1e-05\n",
    "epochs = 100\n",
    "learning_rate = 0.001\n",
    "patience = 5\n",
    "batch_size = 10000\n",
    "model = create_nn(3, X_test.shape[1], lamda , learning_rate)\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                        epochs=100, \n",
    "                        batch_size=batch_size, \n",
    "                        verbose=False,\n",
    "                        validation_data = (X_val, y_val),\n",
    "                        callbacks = [EarlyStopping(patience = patience, restore_best_weights=True)])\n",
    "predictions = model.predict(X_test)\n",
    "df_predictions = pd.DataFrame(predictions, columns=['Prediction'])\n",
    "df_predictions['Actual'] = y_test\n",
    "df_predictions['dif_squared'] = (df_predictions['Prediction'] - df_predictions['Actual'])**2\n",
    "df_predictions['actual_sqaured'] = df_predictions['Actual']**2\n",
    "R_OOS = 1 - (df_predictions['dif_squared'].sum()/df_predictions['actual_sqaured'].sum()) \n",
    "print(R_OOS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to calculate R^2 OOS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhammatornriewcharoon/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3088/3088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 231us/step\n",
      "\u001b[1m3088/3088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 237us/step\n",
      "\u001b[1m3088/3088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 227us/step\n",
      "\u001b[1m3088/3088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 226us/step\n",
      "\u001b[1m3088/3088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 227us/step\n",
      "\u001b[1m3088/3088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 229us/step\n",
      "\u001b[1m3088/3088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 226us/step\n",
      "\u001b[1m3088/3088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 229us/step\n",
      "\u001b[1m3088/3088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 235us/step\n",
      "\u001b[1m3088/3088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 229us/step\n",
      "\u001b[1m2884/2884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 236us/step\n",
      "\u001b[1m2884/2884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 244us/step\n",
      "\u001b[1m2884/2884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 233us/step\n",
      "\u001b[1m2884/2884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 231us/step\n",
      "\u001b[1m2884/2884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 243us/step\n",
      "\u001b[1m2884/2884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 232us/step\n",
      "\u001b[1m2884/2884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 227us/step\n",
      "\u001b[1m2884/2884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 233us/step\n",
      "\u001b[1m2884/2884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 232us/step\n",
      "\u001b[1m2884/2884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 280us/step\n",
      "\u001b[1m2688/2688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 261us/step\n",
      "\u001b[1m2688/2688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 223us/step\n",
      "\u001b[1m2688/2688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 225us/step\n",
      "\u001b[1m2688/2688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 223us/step\n",
      "\u001b[1m2688/2688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 238us/step\n",
      "\u001b[1m2688/2688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 225us/step\n",
      "\u001b[1m2688/2688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 226us/step\n",
      "\u001b[1m2688/2688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 224us/step\n",
      "\u001b[1m2688/2688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 229us/step\n",
      "\u001b[1m2688/2688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 225us/step\n",
      "\u001b[1m2535/2535\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 243us/step\n",
      "\u001b[1m2535/2535\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 235us/step\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "end_year = date.max()\n",
    "\n",
    "# Hyperparameters \n",
    "lamda = 1e-05\n",
    "epochs = 100\n",
    "learning_rate = 0.001\n",
    "patience = 5\n",
    "batch_size = 10000\n",
    "ensemble = 10 \n",
    "hidden_layers = 3\n",
    "\n",
    "# Size of rolling window in years \n",
    "val_length = 12 \n",
    "test_length = 1\n",
    "\n",
    "total_R_2_OOS = [] \n",
    "model = create_nn(hidden_layers, X.shape[1], lamda , learning_rate)\n",
    "\n",
    "for i in range(1988, end_year.year - val_length - test_length):\n",
    "    print(i)\n",
    "    predictions = [] \n",
    "    training_window = (date < datetime(i,1,1)) \n",
    "    validation_window = (date >= datetime(i,1,1)) & (date < datetime(i+val_length,1,1)) \n",
    "    test_window = (date >= datetime(i+val_length,1,1)) & (date < datetime(i+val_length+test_length,1,1))\n",
    "\n",
    "    X_train_expanding, y_train_expanding = X.loc[training_window].values, y.loc[training_window].values\n",
    "    X_val_expanding , y_val_expanding = X.loc[validation_window].values, y.loc[validation_window].values\n",
    "    X_test_expanding, y_test_expanding =  X.loc[test_window].values, y.loc[test_window].values\n",
    "\n",
    "    # take the final output as the average of ensemble number of NN \n",
    "    for j in range(ensemble): \n",
    "        # set random seed each time the model is trained \n",
    "        seed = int(time.time())\n",
    "        tf.random.set_seed(seed)\n",
    "        history = model.fit(X_train_expanding, y_train_expanding, \n",
    "                            epochs=epochs, \n",
    "                            batch_size=batch_size, \n",
    "                            verbose=False,\n",
    "                            validation_data = (X_val_expanding, y_val_expanding),\n",
    "                            callbacks = [EarlyStopping(patience = patience, restore_best_weights=True)])\n",
    "        current_prediction = model.predict(X_test_expanding)\n",
    "        # Average the predictions 10 times from 10 different nueral network models \n",
    "        if len(predictions) == 0:\n",
    "            predictions = current_prediction\n",
    "        else: \n",
    "            predictions = (predictions + current_prediction) /2 \n",
    "\n",
    "    df_predictions = pd.DataFrame(predictions, columns=['Prediction'])\n",
    "    df_predictions['Actual'] = y_test_expanding\n",
    "    df_predictions['dif_squared'] = (df_predictions['Prediction'] - df_predictions['Actual'])**2\n",
    "    df_predictions['actual_sqaured'] = df_predictions['Actual']**2\n",
    "    R_OOS = 1 - (df_predictions['dif_squared'].sum()/df_predictions['actual_sqaured'].sum()) \n",
    "    # print(\"***** R^2_OOS \", i, R_OOS)\n",
    "    total_R_2_OOS.append(R_OOS)\n",
    "\n",
    "\n",
    "# calculate the mean OOS for all time periods \n",
    "print(\"Final R^2 OOS :  \", np.mean(total_R_2_OOS)) \n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
