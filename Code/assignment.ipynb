{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import optuna\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(999)\n",
    "from tensorflow.keras.regularizers import L1\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,BatchNormalization,Activation\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "\n",
    "panel = pd.read_pickle('../Data/returns_chars_panel.pkl') \n",
    "macro = pd.read_pickle('../Data/macro_timeseries.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine micro and macro data\n",
    "df = pd.merge(panel,macro,on='date',how='left',suffixes=['','_macro']) \n",
    "\n",
    "# features + targets \n",
    "X = df.drop(columns=['ret','excess_ret','rfree','permno','date']) # everything except return info and IDs\n",
    "y = df['excess_ret'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Train Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make 20 years of training data\n",
    "date = df['date']\n",
    "training = (date <= '1977-03') # selects \n",
    "X_train, y_train = X.loc[training].values, y.loc[training].values \n",
    "\n",
    "# make 10 years of validation data\n",
    "validation = (date > '1977-03') & (date <= '1987-03') \n",
    "X_val, y_val = X.loc[validation].values, y.loc[validation].values \n",
    "\n",
    "# make test data\n",
    "test = (date > '1987-03') \n",
    "X_test, y_test = X.loc[test].values, y.loc[test].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lamda = 1e-5\n",
    "epochs = 100\n",
    "# learning_rate = 0.0001\n",
    "patience = 5\n",
    "batch_size = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding baseline model for Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the final model \n",
    "def create_nn(n_layers, input_dim, lamda, learning_rate):\n",
    "    first_layer = 32\n",
    "    num_layers = 32 \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_dim=input_dim, \n",
    "                kernel_regularizer=regularizers.L1(lamda), \n",
    "                kernel_initializer = 'he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # add extra hidden layers \n",
    "    for i in range(n_layers - 1): \n",
    "        num_layers = int(num_layers / 2)\n",
    "        print(num_layers)\n",
    "        model.add(Dense(num_layers,\n",
    "                kernel_regularizer=regularizers.L1(lamda), \n",
    "                kernel_initializer = 'he_normal'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "    # output layer \n",
    "    model.add(Dense(1, activation='linear',\n",
    "                    kernel_regularizer=regularizers.L1(0.01), \n",
    "                    kernel_initializer = 'he_normal'))\n",
    "\n",
    "    model.compile(loss='mse', \n",
    "                optimizer=optimizer,\n",
    "                metrics = ['mse']) \n",
    "    return model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation for Lamda for L2 Regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 09:51:22,255] A new study created in memory with name: no-name-3f083e90-4b67-457e-8315-1fe24896cbbe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 09:51:53,688] Trial 0 finished with value: 0.02663322351872921 and parameters: {'learning_rate': 0.005884424119274088, 'l1_reg': 1.5027262158789137e-05}. Best is trial 0 with value: 0.02663322351872921.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 09:52:35,638] Trial 1 finished with value: 0.026709742844104767 and parameters: {'learning_rate': 0.0017210962358792364, 'l1_reg': 9.376859223285416e-05}. Best is trial 0 with value: 0.02663322351872921.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 09:53:01,404] Trial 2 finished with value: 0.02648334763944149 and parameters: {'learning_rate': 0.0014705134062547398, 'l1_reg': 1.649558851353367e-05}. Best is trial 2 with value: 0.02648334763944149.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 09:54:06,528] Trial 3 finished with value: 0.0268534068018198 and parameters: {'learning_rate': 0.009877711158990365, 'l1_reg': 2.2540953606748685e-05}. Best is trial 2 with value: 0.02648334763944149.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 09:54:29,955] Trial 4 finished with value: 0.026836102828383446 and parameters: {'learning_rate': 0.004063465287472252, 'l1_reg': 0.00017841762918646364}. Best is trial 2 with value: 0.02648334763944149.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 09:54:50,887] Trial 5 finished with value: 0.026734134182333946 and parameters: {'learning_rate': 0.0033445223041875948, 'l1_reg': 0.0001735058350691107}. Best is trial 2 with value: 0.02648334763944149.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 09:55:10,398] Trial 6 finished with value: 0.027333185076713562 and parameters: {'learning_rate': 0.006004271376997853, 'l1_reg': 0.00037495134301516114}. Best is trial 2 with value: 0.02648334763944149.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 09:55:29,276] Trial 7 finished with value: 0.026592226698994637 and parameters: {'learning_rate': 0.00295393519069072, 'l1_reg': 0.000152413821510078}. Best is trial 2 with value: 0.02648334763944149.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 09:56:09,281] Trial 8 finished with value: 0.026458438485860825 and parameters: {'learning_rate': 0.0020114891212278073, 'l1_reg': 1.965055174486702e-05}. Best is trial 8 with value: 0.026458438485860825.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 09:56:30,779] Trial 9 finished with value: 0.026772230863571167 and parameters: {'learning_rate': 0.00575191729726104, 'l1_reg': 0.00015495480138570146}. Best is trial 8 with value: 0.026458438485860825.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 09:56:55,600] Trial 10 finished with value: 0.026645846664905548 and parameters: {'learning_rate': 0.0021279007665057875, 'l1_reg': 4.3440896219815514e-05}. Best is trial 8 with value: 0.026458438485860825.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-05-24 09:57:10,985] Trial 11 failed with parameters: {'learning_rate': 0.0010030927962998086, 'l1_reg': 1.1113557467250788e-05} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/dhammatornriewcharoon/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/var/folders/df/x7ngsr_15bx8pln6jsqgdxy00000gn/T/ipykernel_2523/3233190141.py\", line 43, in objective\n",
      "    history = model.fit(\n",
      "              ^^^^^^^^^^\n",
      "  File \"/Users/dhammatornriewcharoon/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/dhammatornriewcharoon/anaconda3/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py\", line 314, in fit\n",
      "    logs = self.train_function(iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/dhammatornriewcharoon/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/dhammatornriewcharoon/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 833, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/dhammatornriewcharoon/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 878, in _call\n",
      "    results = tracing_compilation.call_function(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/dhammatornriewcharoon/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\", line 139, in call_function\n",
      "    return function._call_flat(  # pylint: disable=protected-access\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/dhammatornriewcharoon/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\", line 1322, in _call_flat\n",
      "    return self._inference_function.call_preflattened(args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/dhammatornriewcharoon/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\", line 216, in call_preflattened\n",
      "    flat_outputs = self.call_flat(*args)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/dhammatornriewcharoon/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\", line 251, in call_flat\n",
      "    outputs = self._bound_context.call_function(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/dhammatornriewcharoon/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/context.py\", line 1500, in call_function\n",
      "    outputs = execute.execute(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/dhammatornriewcharoon/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/execute.py\", line 53, in quick_execute\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2024-05-24 09:57:10,990] Trial 11 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 58\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Create a study and optimize the objective function\u001b[39;00m\n\u001b[1;32m     57\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(objective, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Get the best trial\u001b[39;00m\n\u001b[1;32m     61\u001b[0m best_trial \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_trial\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     _optimize(\n\u001b[1;32m    452\u001b[0m         study\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    453\u001b[0m         func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m    454\u001b[0m         n_trials\u001b[38;5;241m=\u001b[39mn_trials,\n\u001b[1;32m    455\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    456\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[1;32m    457\u001b[0m         catch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(catch) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(catch, Iterable) \u001b[38;5;28;01melse\u001b[39;00m (catch,),\n\u001b[1;32m    458\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    459\u001b[0m         gc_after_trial\u001b[38;5;241m=\u001b[39mgc_after_trial,\n\u001b[1;32m    460\u001b[0m         show_progress_bar\u001b[38;5;241m=\u001b[39mshow_progress_bar,\n\u001b[1;32m    461\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m         _optimize_sequential(\n\u001b[1;32m     63\u001b[0m             study,\n\u001b[1;32m     64\u001b[0m             func,\n\u001b[1;32m     65\u001b[0m             n_trials,\n\u001b[1;32m     66\u001b[0m             timeout,\n\u001b[1;32m     67\u001b[0m             catch,\n\u001b[1;32m     68\u001b[0m             callbacks,\n\u001b[1;32m     69\u001b[0m             gc_after_trial,\n\u001b[1;32m     70\u001b[0m             reseed_sampler_rng\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     71\u001b[0m             time_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     72\u001b[0m             progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[1;32m     73\u001b[0m         )\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    243\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    246\u001b[0m ):\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m func(trial)\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[90], line 43\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Use early stopping\u001b[39;00m\n\u001b[1;32m     41\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(patience\u001b[38;5;241m=\u001b[39mpatience, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 43\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     44\u001b[0m     X_train, y_train,\n\u001b[1;32m     45\u001b[0m     epochs,\n\u001b[1;32m     46\u001b[0m     batch_size,\n\u001b[1;32m     47\u001b[0m     validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m     48\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[early_stopping],\n\u001b[1;32m     49\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     50\u001b[0m )\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     53\u001b[0m score \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_val, y_val, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:314\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    313\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 314\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m    315\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    316\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m    879\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[1;32m    880\u001b[0m )\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[1;32m    141\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[1;32m    255\u001b[0m     )\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[1;32m   1501\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1502\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   1503\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[1;32m   1504\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[1;32m   1505\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1506\u001b[0m   )\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1515\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Using Optuna to cross validate hyper parameter \n",
    "input_dim = X_train.shape[1]\n",
    "n_layers = 2\n",
    "def create_model(trial):\n",
    "\n",
    "    num_layers = 32 \n",
    "    # Suggest hyperparameters\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.001, 0.01, log=True)\n",
    "    l1_reg = trial.suggest_float('l1_reg', 1e-5, 1e-3, log=True)\n",
    "\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_dim=input_dim, \n",
    "                kernel_regularizer=regularizers.L1(l1_reg), \n",
    "                kernel_initializer = 'he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # add extra hidden layers \n",
    "    for i in range(n_layers - 1): \n",
    "        num_layers = int(num_layers / 2)\n",
    "        print(num_layers)\n",
    "        model.add(Dense(num_layers,\n",
    "                kernel_regularizer=regularizers.L1(l1_reg), \n",
    "                kernel_initializer = 'he_normal'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "    # output layer \n",
    "    model.add(Dense(1, activation='linear',\n",
    "                    kernel_regularizer=regularizers.L1(0.01), \n",
    "                    kernel_initializer = 'he_normal'))\n",
    "\n",
    "    model.compile(loss='mse', \n",
    "                optimizer=optimizer,\n",
    "                metrics = ['mse']) \n",
    "    return model\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial):\n",
    "    model = create_model(trial)\n",
    "    \n",
    "    # Use early stopping\n",
    "    early_stopping = EarlyStopping(patience=patience, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs,\n",
    "        batch_size,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Evaluate the model\n",
    "    score = model.evaluate(X_val, y_val, verbose=0)\n",
    "    return score[0]\n",
    "\n",
    "# Create a study and optimize the objective function\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Get the best trial\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(\"Best trial:\")\n",
    "print(f\"  Value: {best_trial.value}\")\n",
    "print(\"  Params: \")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhammatornriewcharoon/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "4\n",
      "Epoch 1/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.3309 - mse: 1.3050 - val_loss: 1.2189 - val_mse: 1.1931\n",
      "Epoch 2/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.7095 - mse: 0.6837 - val_loss: 0.5933 - val_mse: 0.5676\n",
      "Epoch 3/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.3662 - mse: 0.3404 - val_loss: 0.3846 - val_mse: 0.3589\n",
      "Epoch 4/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.2110 - mse: 0.1853 - val_loss: 0.2652 - val_mse: 0.2395\n",
      "Epoch 5/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.1391 - mse: 0.1134 - val_loss: 0.1887 - val_mse: 0.1630\n",
      "Epoch 6/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.1024 - mse: 0.0767 - val_loss: 0.1414 - val_mse: 0.1157\n",
      "Epoch 7/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0817 - mse: 0.0560 - val_loss: 0.1125 - val_mse: 0.0868\n",
      "Epoch 8/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0694 - mse: 0.0437 - val_loss: 0.0946 - val_mse: 0.0689\n",
      "Epoch 9/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0617 - mse: 0.0360 - val_loss: 0.0832 - val_mse: 0.0575\n",
      "Epoch 10/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0567 - mse: 0.0310 - val_loss: 0.0756 - val_mse: 0.0499\n",
      "Epoch 11/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0533 - mse: 0.0276 - val_loss: 0.0703 - val_mse: 0.0446\n",
      "Epoch 12/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0509 - mse: 0.0252 - val_loss: 0.0666 - val_mse: 0.0409\n",
      "Epoch 13/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0491 - mse: 0.0235 - val_loss: 0.0638 - val_mse: 0.0382\n",
      "Epoch 14/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0479 - mse: 0.0222 - val_loss: 0.0617 - val_mse: 0.0361\n",
      "Epoch 15/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0469 - mse: 0.0212 - val_loss: 0.0601 - val_mse: 0.0345\n",
      "Epoch 16/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0462 - mse: 0.0205 - val_loss: 0.0589 - val_mse: 0.0332\n",
      "Epoch 17/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0456 - mse: 0.0200 - val_loss: 0.0579 - val_mse: 0.0323\n",
      "Epoch 18/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0451 - mse: 0.0195 - val_loss: 0.0571 - val_mse: 0.0315\n",
      "Epoch 19/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0448 - mse: 0.0192 - val_loss: 0.0565 - val_mse: 0.0309\n",
      "Epoch 20/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0445 - mse: 0.0189 - val_loss: 0.0560 - val_mse: 0.0304\n",
      "Epoch 21/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0442 - mse: 0.0187 - val_loss: 0.0556 - val_mse: 0.0300\n",
      "Epoch 22/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0440 - mse: 0.0185 - val_loss: 0.0552 - val_mse: 0.0297\n",
      "Epoch 23/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0439 - mse: 0.0184 - val_loss: 0.0550 - val_mse: 0.0294\n",
      "Epoch 24/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0438 - mse: 0.0183 - val_loss: 0.0547 - val_mse: 0.0292\n",
      "Epoch 25/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0437 - mse: 0.0182 - val_loss: 0.0546 - val_mse: 0.0291\n",
      "Epoch 26/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0436 - mse: 0.0181 - val_loss: 0.0544 - val_mse: 0.0290\n",
      "Epoch 27/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0435 - mse: 0.0180 - val_loss: 0.0543 - val_mse: 0.0289\n",
      "Epoch 28/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0434 - mse: 0.0180 - val_loss: 0.0542 - val_mse: 0.0288\n",
      "Epoch 29/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0434 - mse: 0.0180 - val_loss: 0.0542 - val_mse: 0.0288\n",
      "Epoch 30/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0433 - mse: 0.0179 - val_loss: 0.0541 - val_mse: 0.0287\n",
      "Epoch 31/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0433 - mse: 0.0179 - val_loss: 0.0540 - val_mse: 0.0286\n",
      "Epoch 32/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0432 - mse: 0.0179 - val_loss: 0.0539 - val_mse: 0.0285\n",
      "Epoch 33/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0432 - mse: 0.0179 - val_loss: 0.0538 - val_mse: 0.0285\n",
      "Epoch 34/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0432 - mse: 0.0179 - val_loss: 0.0537 - val_mse: 0.0285\n",
      "Epoch 35/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0431 - mse: 0.0179 - val_loss: 0.0537 - val_mse: 0.0284\n",
      "Epoch 36/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0431 - mse: 0.0179 - val_loss: 0.0536 - val_mse: 0.0284\n",
      "Epoch 37/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0431 - mse: 0.0179 - val_loss: 0.0535 - val_mse: 0.0283\n",
      "Epoch 38/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0431 - mse: 0.0178 - val_loss: 0.0534 - val_mse: 0.0282\n",
      "Epoch 39/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0430 - mse: 0.0178 - val_loss: 0.0533 - val_mse: 0.0281\n",
      "Epoch 40/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0430 - mse: 0.0178 - val_loss: 0.0531 - val_mse: 0.0280\n",
      "Epoch 41/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0430 - mse: 0.0178 - val_loss: 0.0530 - val_mse: 0.0279\n",
      "Epoch 42/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0429 - mse: 0.0178 - val_loss: 0.0528 - val_mse: 0.0277\n",
      "Epoch 43/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0429 - mse: 0.0178 - val_loss: 0.0526 - val_mse: 0.0276\n",
      "Epoch 44/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0429 - mse: 0.0178 - val_loss: 0.0525 - val_mse: 0.0275\n",
      "Epoch 45/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0428 - mse: 0.0178 - val_loss: 0.0523 - val_mse: 0.0274\n",
      "Epoch 46/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0428 - mse: 0.0178 - val_loss: 0.0522 - val_mse: 0.0273\n",
      "Epoch 47/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0428 - mse: 0.0178 - val_loss: 0.0521 - val_mse: 0.0272\n",
      "Epoch 48/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0427 - mse: 0.0178 - val_loss: 0.0520 - val_mse: 0.0271\n",
      "Epoch 49/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0427 - mse: 0.0178 - val_loss: 0.0519 - val_mse: 0.0270\n",
      "Epoch 50/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0427 - mse: 0.0178 - val_loss: 0.0518 - val_mse: 0.0269\n",
      "Epoch 51/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0426 - mse: 0.0178 - val_loss: 0.0517 - val_mse: 0.0269\n",
      "Epoch 52/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0426 - mse: 0.0178 - val_loss: 0.0516 - val_mse: 0.0268\n",
      "Epoch 53/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0426 - mse: 0.0178 - val_loss: 0.0515 - val_mse: 0.0268\n",
      "Epoch 54/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0425 - mse: 0.0178 - val_loss: 0.0514 - val_mse: 0.0267\n",
      "Epoch 55/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0425 - mse: 0.0178 - val_loss: 0.0513 - val_mse: 0.0267\n",
      "Epoch 56/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0425 - mse: 0.0178 - val_loss: 0.0512 - val_mse: 0.0266\n",
      "Epoch 57/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0424 - mse: 0.0178 - val_loss: 0.0511 - val_mse: 0.0266\n",
      "Epoch 58/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0424 - mse: 0.0178 - val_loss: 0.0511 - val_mse: 0.0265\n",
      "Epoch 59/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0423 - mse: 0.0178 - val_loss: 0.0510 - val_mse: 0.0265\n",
      "Epoch 60/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0423 - mse: 0.0178 - val_loss: 0.0509 - val_mse: 0.0265\n",
      "Epoch 61/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0422 - mse: 0.0178 - val_loss: 0.0509 - val_mse: 0.0265\n",
      "Epoch 62/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0422 - mse: 0.0178 - val_loss: 0.0508 - val_mse: 0.0265\n",
      "Epoch 63/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0422 - mse: 0.0178 - val_loss: 0.0507 - val_mse: 0.0264\n",
      "Epoch 64/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0421 - mse: 0.0178 - val_loss: 0.0507 - val_mse: 0.0264\n",
      "Epoch 65/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0421 - mse: 0.0178 - val_loss: 0.0506 - val_mse: 0.0264\n",
      "Epoch 66/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0420 - mse: 0.0178 - val_loss: 0.0506 - val_mse: 0.0264\n",
      "Epoch 67/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0420 - mse: 0.0178 - val_loss: 0.0505 - val_mse: 0.0264\n",
      "Epoch 68/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0419 - mse: 0.0178 - val_loss: 0.0505 - val_mse: 0.0264\n",
      "Epoch 69/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0419 - mse: 0.0178 - val_loss: 0.0504 - val_mse: 0.0264\n",
      "Epoch 70/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0418 - mse: 0.0178 - val_loss: 0.0503 - val_mse: 0.0264\n",
      "Epoch 71/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0418 - mse: 0.0178 - val_loss: 0.0503 - val_mse: 0.0264\n",
      "Epoch 72/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0417 - mse: 0.0178 - val_loss: 0.0502 - val_mse: 0.0264\n",
      "Epoch 73/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0416 - mse: 0.0178 - val_loss: 0.0501 - val_mse: 0.0264\n",
      "Epoch 74/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0416 - mse: 0.0178 - val_loss: 0.0501 - val_mse: 0.0264\n",
      "Epoch 75/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0415 - mse: 0.0178 - val_loss: 0.0500 - val_mse: 0.0263\n",
      "Epoch 76/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0415 - mse: 0.0178 - val_loss: 0.0499 - val_mse: 0.0263\n",
      "Epoch 77/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0414 - mse: 0.0178 - val_loss: 0.0499 - val_mse: 0.0263\n",
      "Epoch 78/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0413 - mse: 0.0178 - val_loss: 0.0498 - val_mse: 0.0263\n",
      "Epoch 79/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0413 - mse: 0.0178 - val_loss: 0.0497 - val_mse: 0.0263\n",
      "Epoch 80/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0412 - mse: 0.0178 - val_loss: 0.0496 - val_mse: 0.0263\n",
      "Epoch 81/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0411 - mse: 0.0178 - val_loss: 0.0496 - val_mse: 0.0263\n",
      "Epoch 82/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0411 - mse: 0.0178 - val_loss: 0.0495 - val_mse: 0.0263\n",
      "Epoch 83/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0410 - mse: 0.0178 - val_loss: 0.0494 - val_mse: 0.0263\n",
      "Epoch 84/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0409 - mse: 0.0178 - val_loss: 0.0493 - val_mse: 0.0263\n",
      "Epoch 85/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0408 - mse: 0.0178 - val_loss: 0.0493 - val_mse: 0.0263\n",
      "Epoch 86/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0408 - mse: 0.0178 - val_loss: 0.0492 - val_mse: 0.0263\n",
      "Epoch 87/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0407 - mse: 0.0178 - val_loss: 0.0491 - val_mse: 0.0263\n",
      "Epoch 88/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0406 - mse: 0.0178 - val_loss: 0.0490 - val_mse: 0.0263\n",
      "Epoch 89/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0405 - mse: 0.0178 - val_loss: 0.0489 - val_mse: 0.0263\n",
      "Epoch 90/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0404 - mse: 0.0178 - val_loss: 0.0488 - val_mse: 0.0263\n",
      "Epoch 91/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0404 - mse: 0.0178 - val_loss: 0.0487 - val_mse: 0.0263\n",
      "Epoch 92/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0403 - mse: 0.0178 - val_loss: 0.0487 - val_mse: 0.0263\n",
      "Epoch 93/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0402 - mse: 0.0178 - val_loss: 0.0486 - val_mse: 0.0263\n",
      "Epoch 94/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0401 - mse: 0.0178 - val_loss: 0.0485 - val_mse: 0.0263\n",
      "Epoch 95/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0400 - mse: 0.0178 - val_loss: 0.0484 - val_mse: 0.0262\n",
      "Epoch 96/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0399 - mse: 0.0178 - val_loss: 0.0483 - val_mse: 0.0262\n",
      "Epoch 97/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0398 - mse: 0.0178 - val_loss: 0.0482 - val_mse: 0.0262\n",
      "Epoch 98/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0397 - mse: 0.0178 - val_loss: 0.0481 - val_mse: 0.0262\n",
      "Epoch 99/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0396 - mse: 0.0178 - val_loss: 0.0480 - val_mse: 0.0262\n",
      "Epoch 100/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0395 - mse: 0.0178 - val_loss: 0.0479 - val_mse: 0.0262\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "model = create_nn(4 , X_train.shape[1], lamda, learning_rate)\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    verbose=True,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[EarlyStopping(patience=patience, restore_best_weights=True)])\n",
    "\n",
    "# Plotting the training and validation MSE\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "pd.Series(history.history['mse']).plot(ax=ax, label='training mse')\n",
    "pd.Series(history.history['val_mse']).plot(ax=ax, label='validation mse')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_R_2_OOS = [] \n",
    "for i in range(1977, 1987):\n",
    "    train_window = (date < datetime(i+1,1,1)) \n",
    "    test_window = (date >= datetime(i,1,1)) & (date < datetime(i+1,1,1)) \n",
    "    # print('Train', np.sum(train_window))\n",
    "    # print('Test', np.sum(test_window))\n",
    "    X_train_expanding, y_train_expanding = X.loc[train_window].values, y.loc[train_window].values\n",
    "    X_test_expanding, y_test_expanding =  X.loc[test_window].values, y.loc[test_window].values\n",
    "\n",
    "    lamda = 1e-5\n",
    "    epochs = 100\n",
    "    learning_rate = 0.0001\n",
    "    patience = 5\n",
    "    batch_size = 10000\n",
    "\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model = nn3(lamda)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mse'])\n",
    "    history = model.fit(X_train_expanding, y_train_expanding, \n",
    "                        epochs=epochs, \n",
    "                        batch_size=batch_size, \n",
    "                        verbose=True,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        callbacks=[EarlyStopping(patience=patience, restore_best_weights=True)])\n",
    "\n",
    "    # Plotting the training and validation MSE\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    pd.Series(history.history['mse']).plot(ax=ax, label='training mse')\n",
    "    pd.Series(history.history['val_mse']).plot(ax=ax, label='validation mse')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "    predictions = model.predict(X_val)\n",
    "    print(predictions)\n",
    "    df_predictions = pd.DataFrame(predictions, columns=['Prediction'])\n",
    "    df_predictions['Actual'] = y_val\n",
    "    df_predictions['dif_squared'] = (df_predictions['Prediction'] - df_predictions['Actual'])**2\n",
    "    df_predictions['actual_sqaured'] = df_predictions['Actual']**2\n",
    "    R_OOS = 1 - (df_predictions['dif_squared'].sum()/df_predictions['actual_sqaured'].sum()) \n",
    "    R_OOS\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling Window R^2_OOS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1988\n",
      "Epoch 1/100\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.5509 - mse: 0.5499 - val_loss: 0.0569 - val_mse: 0.0559\n",
      "Epoch 2/100\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0477 - mse: 0.0467 - val_loss: 0.0396 - val_mse: 0.0386\n",
      "Epoch 3/100\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0357 - mse: 0.0347 - val_loss: 0.0344 - val_mse: 0.0334\n",
      "Epoch 4/100\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0326 - mse: 0.0316 - val_loss: 0.0318 - val_mse: 0.0309\n",
      "Epoch 5/100\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0313 - mse: 0.0303 - val_loss: 0.0305 - val_mse: 0.0296\n",
      "Epoch 6/100\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0306 - mse: 0.0297 - val_loss: 0.0297 - val_mse: 0.0288\n",
      "Epoch 7/100\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0302 - mse: 0.0293 - val_loss: 0.0293 - val_mse: 0.0284\n",
      "Epoch 8/100\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0299 - mse: 0.0290 - val_loss: 0.0291 - val_mse: 0.0282\n",
      "Epoch 9/100\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0297 - mse: 0.0287 - val_loss: 0.0292 - val_mse: 0.0283\n",
      "Epoch 10/100\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0294 - mse: 0.0285 - val_loss: 0.0299 - val_mse: 0.0289\n",
      "\u001b[1m2631/2631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 222us/step\n",
      "1989\n",
      "Epoch 1/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0298 - mse: 0.0289 - val_loss: 0.0295 - val_mse: 0.0286\n",
      "Epoch 2/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0294 - mse: 0.0285 - val_loss: 0.0316 - val_mse: 0.0307\n",
      "Epoch 3/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0291 - mse: 0.0283 - val_loss: 0.0356 - val_mse: 0.0347\n",
      "\u001b[1m2558/2558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 215us/step\n",
      "1990\n",
      "Epoch 1/100\n",
      "\u001b[1m605/605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0313 - mse: 0.0304 - val_loss: 0.0321 - val_mse: 0.0312\n",
      "Epoch 2/100\n",
      "\u001b[1m605/605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0309 - mse: 0.0301 - val_loss: 0.0353 - val_mse: 0.0344\n",
      "Epoch 3/100\n",
      "\u001b[1m605/605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0307 - mse: 0.0299 - val_loss: 0.0372 - val_mse: 0.0364\n",
      "\u001b[1m2525/2525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 223us/step\n",
      "1991\n",
      "Epoch 1/100\n",
      "\u001b[1m761/761\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0348 - mse: 0.0339 - val_loss: 0.0364 - val_mse: 0.0356\n",
      "Epoch 2/100\n",
      "\u001b[1m761/761\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0344 - mse: 0.0336 - val_loss: 0.0393 - val_mse: 0.0385\n",
      "Epoch 3/100\n",
      "\u001b[1m761/761\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0342 - mse: 0.0334 - val_loss: 0.0390 - val_mse: 0.0383\n",
      "\u001b[1m2494/2494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 219us/step\n",
      "1992\n",
      "Epoch 1/100\n",
      "\u001b[1m921/921\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0392 - mse: 0.0384 - val_loss: 0.0384 - val_mse: 0.0377\n",
      "Epoch 2/100\n",
      "\u001b[1m921/921\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0389 - mse: 0.0382 - val_loss: 0.0398 - val_mse: 0.0391\n",
      "Epoch 3/100\n",
      "\u001b[1m921/921\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0387 - mse: 0.0381 - val_loss: 0.0382 - val_mse: 0.0376\n",
      "Epoch 4/100\n",
      "\u001b[1m921/921\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0385 - mse: 0.0380 - val_loss: 0.0371 - val_mse: 0.0366\n",
      "Epoch 5/100\n",
      "\u001b[1m921/921\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0383 - mse: 0.0378 - val_loss: 0.0362 - val_mse: 0.0358\n",
      "Epoch 6/100\n",
      "\u001b[1m921/921\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0382 - mse: 0.0377 - val_loss: 0.0342 - val_mse: 0.0338\n",
      "Epoch 7/100\n",
      "\u001b[1m921/921\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0381 - mse: 0.0376 - val_loss: 0.0334 - val_mse: 0.0330\n",
      "Epoch 8/100\n",
      "\u001b[1m921/921\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0379 - mse: 0.0375 - val_loss: 0.0324 - val_mse: 0.0320\n",
      "Epoch 9/100\n",
      "\u001b[1m921/921\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0376 - mse: 0.0371 - val_loss: 0.0317 - val_mse: 0.0312\n",
      "Epoch 10/100\n",
      "\u001b[1m921/921\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0373 - mse: 0.0369 - val_loss: 0.0315 - val_mse: 0.0310\n",
      "Epoch 11/100\n",
      "\u001b[1m921/921\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0374 - mse: 0.0369 - val_loss: 0.0322 - val_mse: 0.0318\n",
      "Epoch 12/100\n",
      "\u001b[1m921/921\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0372 - mse: 0.0368 - val_loss: 0.0326 - val_mse: 0.0322\n",
      "\u001b[1m2551/2551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 213us/step\n",
      "1993\n",
      "Epoch 1/100\n",
      "\u001b[1m1090/1090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0347 - mse: 0.0342 - val_loss: 0.0282 - val_mse: 0.0278\n",
      "Epoch 2/100\n",
      "\u001b[1m1090/1090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 991us/step - loss: 0.0348 - mse: 0.0343 - val_loss: 0.0281 - val_mse: 0.0276\n",
      "Epoch 3/100\n",
      "\u001b[1m1090/1090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 971us/step - loss: 0.0343 - mse: 0.0339 - val_loss: 0.0273 - val_mse: 0.0269\n",
      "Epoch 4/100\n",
      "\u001b[1m1090/1090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0346 - mse: 0.0342 - val_loss: 0.0280 - val_mse: 0.0276\n",
      "Epoch 5/100\n",
      "\u001b[1m1090/1090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 993us/step - loss: 0.0343 - mse: 0.0339 - val_loss: 0.0278 - val_mse: 0.0274\n",
      "\u001b[1m2705/2705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 215us/step\n",
      "1994\n",
      "Epoch 1/100\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0347 - mse: 0.0343 - val_loss: 0.0279 - val_mse: 0.0275\n",
      "Epoch 2/100\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 918us/step - loss: 0.0345 - mse: 0.0341 - val_loss: 0.0280 - val_mse: 0.0277\n",
      "Epoch 3/100\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 909us/step - loss: 0.0345 - mse: 0.0342 - val_loss: 0.0281 - val_mse: 0.0278\n",
      "\u001b[1m2989/2989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 216us/step\n",
      "1995\n",
      "Epoch 1/100\n",
      "\u001b[1m1469/1469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0325 - mse: 0.0321 - val_loss: 0.0276 - val_mse: 0.0272\n",
      "Epoch 2/100\n",
      "\u001b[1m1469/1469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 934us/step - loss: 0.0324 - mse: 0.0320 - val_loss: 0.0277 - val_mse: 0.0273\n",
      "Epoch 3/100\n",
      "\u001b[1m1469/1469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0320 - mse: 0.0316 - val_loss: 0.0278 - val_mse: 0.0274\n",
      "\u001b[1m3075/3075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 217us/step\n",
      "1996\n",
      "Epoch 1/100\n",
      "\u001b[1m1672/1672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0325 - mse: 0.0321 - val_loss: 0.0267 - val_mse: 0.0263\n",
      "Epoch 2/100\n",
      "\u001b[1m1672/1672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 886us/step - loss: 0.0323 - mse: 0.0319 - val_loss: 0.0266 - val_mse: 0.0262\n",
      "Epoch 3/100\n",
      "\u001b[1m1672/1672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 928us/step - loss: 0.0323 - mse: 0.0319 - val_loss: 0.0265 - val_mse: 0.0261\n",
      "Epoch 4/100\n",
      "\u001b[1m1672/1672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 896us/step - loss: 0.0321 - mse: 0.0317 - val_loss: 0.0266 - val_mse: 0.0262\n",
      "Epoch 5/100\n",
      "\u001b[1m1672/1672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 876us/step - loss: 0.0321 - mse: 0.0317 - val_loss: 0.0266 - val_mse: 0.0263\n",
      "\u001b[1m3248/3248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 216us/step\n",
      "1997\n",
      "Epoch 1/100\n",
      "\u001b[1m1883/1883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0324 - mse: 0.0321 - val_loss: 0.0276 - val_mse: 0.0272\n",
      "Epoch 2/100\n",
      "\u001b[1m1883/1883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 857us/step - loss: 0.0323 - mse: 0.0320 - val_loss: 0.0272 - val_mse: 0.0268\n",
      "Epoch 3/100\n",
      "\u001b[1m1883/1883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 864us/step - loss: 0.0324 - mse: 0.0320 - val_loss: 0.0272 - val_mse: 0.0269\n",
      "Epoch 4/100\n",
      "\u001b[1m1883/1883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 863us/step - loss: 0.0323 - mse: 0.0319 - val_loss: 0.0270 - val_mse: 0.0266\n",
      "Epoch 5/100\n",
      "\u001b[1m1883/1883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 936us/step - loss: 0.0325 - mse: 0.0322 - val_loss: 0.0269 - val_mse: 0.0266\n",
      "Epoch 6/100\n",
      "\u001b[1m1883/1883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0322 - mse: 0.0318 - val_loss: 0.0268 - val_mse: 0.0265\n",
      "Epoch 7/100\n",
      "\u001b[1m1883/1883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 846us/step - loss: 0.0322 - mse: 0.0319 - val_loss: 0.0270 - val_mse: 0.0267\n",
      "Epoch 8/100\n",
      "\u001b[1m1883/1883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 892us/step - loss: 0.0321 - mse: 0.0318 - val_loss: 0.0271 - val_mse: 0.0267\n",
      "\u001b[1m3376/3376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 211us/step\n",
      "1998\n",
      "Epoch 1/100\n",
      "\u001b[1m2091/2091\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0332 - mse: 0.0329 - val_loss: 0.0273 - val_mse: 0.0269\n",
      "Epoch 2/100\n",
      "\u001b[1m2091/2091\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 806us/step - loss: 0.0331 - mse: 0.0327 - val_loss: 0.0273 - val_mse: 0.0269\n",
      "Epoch 3/100\n",
      "\u001b[1m2091/2091\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 941us/step - loss: 0.0331 - mse: 0.0327 - val_loss: 0.0275 - val_mse: 0.0271\n",
      "\u001b[1m3337/3337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 230us/step\n",
      "1999\n",
      "Epoch 1/100\n",
      "\u001b[1m2288/2288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 984us/step - loss: 0.0366 - mse: 0.0362 - val_loss: 0.0268 - val_mse: 0.0264\n",
      "Epoch 2/100\n",
      "\u001b[1m2288/2288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 786us/step - loss: 0.0367 - mse: 0.0363 - val_loss: 0.0267 - val_mse: 0.0264\n",
      "Epoch 3/100\n",
      "\u001b[1m2288/2288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 775us/step - loss: 0.0366 - mse: 0.0363 - val_loss: 0.0271 - val_mse: 0.0268\n",
      "Epoch 4/100\n",
      "\u001b[1m2288/2288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 779us/step - loss: 0.0365 - mse: 0.0362 - val_loss: 0.0270 - val_mse: 0.0266\n",
      "\u001b[1m3155/3155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 215us/step\n",
      "2000\n",
      "Epoch 1/100\n",
      "\u001b[1m2481/2481\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 966us/step - loss: 0.0373 - mse: 0.0370 - val_loss: 0.0267 - val_mse: 0.0264\n",
      "Epoch 2/100\n",
      "\u001b[1m2481/2481\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 812us/step - loss: 0.0373 - mse: 0.0370 - val_loss: 0.0266 - val_mse: 0.0263\n",
      "Epoch 3/100\n",
      "\u001b[1m2481/2481\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 840us/step - loss: 0.0373 - mse: 0.0369 - val_loss: 0.0265 - val_mse: 0.0262\n",
      "Epoch 4/100\n",
      "\u001b[1m2481/2481\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 841us/step - loss: 0.0372 - mse: 0.0369 - val_loss: 0.0265 - val_mse: 0.0262\n",
      "Epoch 5/100\n",
      "\u001b[1m2481/2481\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 835us/step - loss: 0.0372 - mse: 0.0369 - val_loss: 0.0265 - val_mse: 0.0262\n",
      "Epoch 6/100\n",
      "\u001b[1m2481/2481\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 881us/step - loss: 0.0372 - mse: 0.0369 - val_loss: 0.0266 - val_mse: 0.0262\n",
      "Epoch 7/100\n",
      "\u001b[1m2481/2481\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 835us/step - loss: 0.0373 - mse: 0.0370 - val_loss: 0.0265 - val_mse: 0.0262\n",
      "\u001b[1m3088/3088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 226us/step\n",
      "2001\n",
      "Epoch 1/100\n",
      "\u001b[1m2661/2661\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 876us/step - loss: 0.0394 - mse: 0.0391 - val_loss: 0.0267 - val_mse: 0.0264\n",
      "Epoch 2/100\n",
      "\u001b[1m2661/2661\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 865us/step - loss: 0.0395 - mse: 0.0392 - val_loss: 0.0267 - val_mse: 0.0264\n",
      "Epoch 3/100\n",
      "\u001b[1m2661/2661\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 833us/step - loss: 0.0395 - mse: 0.0392 - val_loss: 0.0271 - val_mse: 0.0268\n",
      "\u001b[1m2884/2884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 239us/step\n",
      "2002\n",
      "Epoch 1/100\n",
      "\u001b[1m2829/2829\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.0395 - mse: 0.0392 - val_loss: 0.0280 - val_mse: 0.0278\n",
      "Epoch 2/100\n",
      "\u001b[1m2829/2829\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 754us/step - loss: 0.0396 - mse: 0.0393 - val_loss: 0.0283 - val_mse: 0.0280\n",
      "Epoch 3/100\n",
      "\u001b[1m2829/2829\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 732us/step - loss: 0.0394 - mse: 0.0392 - val_loss: 0.0286 - val_mse: 0.0284\n",
      "\u001b[1m2688/2688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 222us/step\n",
      "2003\n",
      "Epoch 1/100\n",
      "\u001b[1m2988/2988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 952us/step - loss: 0.0383 - mse: 0.0380 - val_loss: 0.0273 - val_mse: 0.0270\n",
      "Epoch 2/100\n",
      "\u001b[1m2988/2988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 787us/step - loss: 0.0382 - mse: 0.0379 - val_loss: 0.0273 - val_mse: 0.0270\n",
      "Epoch 3/100\n",
      "\u001b[1m2988/2988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 759us/step - loss: 0.0381 - mse: 0.0379 - val_loss: 0.0267 - val_mse: 0.0264\n",
      "Epoch 4/100\n",
      "\u001b[1m2988/2988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 764us/step - loss: 0.0381 - mse: 0.0379 - val_loss: 0.0291 - val_mse: 0.0288\n",
      "Epoch 5/100\n",
      "\u001b[1m2988/2988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 764us/step - loss: 0.0382 - mse: 0.0379 - val_loss: 0.0271 - val_mse: 0.0268\n",
      "\u001b[1m2535/2535\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 219us/step\n",
      "2004\n",
      "Epoch 1/100\n",
      "\u001b[1m3144/3144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 989us/step - loss: 0.0380 - mse: 0.0377 - val_loss: 0.0270 - val_mse: 0.0267\n",
      "Epoch 2/100\n",
      "\u001b[1m3144/3144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 763us/step - loss: 0.0380 - mse: 0.0377 - val_loss: 0.0272 - val_mse: 0.0269\n",
      "Epoch 3/100\n",
      "\u001b[1m3144/3144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 821us/step - loss: 0.0380 - mse: 0.0378 - val_loss: 0.0272 - val_mse: 0.0269\n",
      "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 232us/step\n",
      "2005\n",
      "Epoch 1/100\n",
      "\u001b[1m3301/3301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 820us/step - loss: 0.0371 - mse: 0.0369 - val_loss: 0.0274 - val_mse: 0.0271\n",
      "Epoch 2/100\n",
      "\u001b[1m3301/3301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 736us/step - loss: 0.0372 - mse: 0.0369 - val_loss: 0.0270 - val_mse: 0.0268\n",
      "Epoch 3/100\n",
      "\u001b[1m3301/3301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 728us/step - loss: 0.0371 - mse: 0.0369 - val_loss: 0.0270 - val_mse: 0.0268\n",
      "Epoch 4/100\n",
      "\u001b[1m3301/3301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 715us/step - loss: 0.0372 - mse: 0.0369 - val_loss: 0.0269 - val_mse: 0.0266\n",
      "Epoch 5/100\n",
      "\u001b[1m3301/3301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 706us/step - loss: 0.0371 - mse: 0.0369 - val_loss: 0.0271 - val_mse: 0.0269\n",
      "Epoch 6/100\n",
      "\u001b[1m3301/3301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 705us/step - loss: 0.0371 - mse: 0.0368 - val_loss: 0.0275 - val_mse: 0.0272\n",
      "\u001b[1m2510/2510\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 215us/step\n",
      "2006\n",
      "Epoch 1/100\n",
      "\u001b[1m3458/3458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 938us/step - loss: 0.0359 - mse: 0.0356 - val_loss: 0.0272 - val_mse: 0.0269\n",
      "Epoch 2/100\n",
      "\u001b[1m3458/3458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 835us/step - loss: 0.0359 - mse: 0.0356 - val_loss: 0.0272 - val_mse: 0.0269\n",
      "Epoch 3/100\n",
      "\u001b[1m3458/3458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 762us/step - loss: 0.0359 - mse: 0.0356 - val_loss: 0.0272 - val_mse: 0.0269\n",
      "Epoch 4/100\n",
      "\u001b[1m3458/3458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 751us/step - loss: 0.0359 - mse: 0.0356 - val_loss: 0.0274 - val_mse: 0.0271\n",
      "Epoch 5/100\n",
      "\u001b[1m3458/3458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 751us/step - loss: 0.0358 - mse: 0.0356 - val_loss: 0.0270 - val_mse: 0.0267\n",
      "Epoch 6/100\n",
      "\u001b[1m3458/3458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 747us/step - loss: 0.0358 - mse: 0.0356 - val_loss: 0.0268 - val_mse: 0.0265\n",
      "Epoch 7/100\n",
      "\u001b[1m3458/3458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 772us/step - loss: 0.0358 - mse: 0.0355 - val_loss: 0.0266 - val_mse: 0.0263\n",
      "Epoch 8/100\n",
      "\u001b[1m3458/3458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 833us/step - loss: 0.0358 - mse: 0.0355 - val_loss: 0.0267 - val_mse: 0.0264\n",
      "Epoch 9/100\n",
      "\u001b[1m3458/3458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 716us/step - loss: 0.0357 - mse: 0.0354 - val_loss: 0.0266 - val_mse: 0.0263\n",
      "Epoch 10/100\n",
      "\u001b[1m3458/3458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 729us/step - loss: 0.0357 - mse: 0.0354 - val_loss: 0.0267 - val_mse: 0.0264\n",
      "Epoch 11/100\n",
      "\u001b[1m3458/3458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 714us/step - loss: 0.0358 - mse: 0.0355 - val_loss: 0.0266 - val_mse: 0.0263\n",
      "\u001b[1m2520/2520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 217us/step\n",
      "2007\n",
      "Epoch 1/100\n",
      "\u001b[1m3618/3618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 913us/step - loss: 0.0351 - mse: 0.0348 - val_loss: 0.0268 - val_mse: 0.0265\n",
      "Epoch 2/100\n",
      "\u001b[1m3618/3618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 913us/step - loss: 0.0351 - mse: 0.0348 - val_loss: 0.0267 - val_mse: 0.0264\n",
      "Epoch 3/100\n",
      "\u001b[1m3618/3618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 719us/step - loss: 0.0352 - mse: 0.0349 - val_loss: 0.0274 - val_mse: 0.0270\n",
      "Epoch 4/100\n",
      "\u001b[1m3618/3618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 741us/step - loss: 0.0351 - mse: 0.0348 - val_loss: 0.0272 - val_mse: 0.0269\n",
      "\u001b[1m2554/2554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 208us/step\n",
      "2008\n",
      "Epoch 1/100\n",
      "\u001b[1m3771/3771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 950us/step - loss: 0.0346 - mse: 0.0343 - val_loss: 0.0273 - val_mse: 0.0270\n",
      "Epoch 2/100\n",
      "\u001b[1m3771/3771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 811us/step - loss: 0.0345 - mse: 0.0342 - val_loss: 0.0278 - val_mse: 0.0275\n",
      "Epoch 3/100\n",
      "\u001b[1m3771/3771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 777us/step - loss: 0.0345 - mse: 0.0342 - val_loss: 0.0269 - val_mse: 0.0266\n",
      "Epoch 4/100\n",
      "\u001b[1m3771/3771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 827us/step - loss: 0.0345 - mse: 0.0342 - val_loss: 0.0268 - val_mse: 0.0265\n",
      "Epoch 5/100\n",
      "\u001b[1m3771/3771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 715us/step - loss: 0.0345 - mse: 0.0341 - val_loss: 0.0267 - val_mse: 0.0264\n",
      "Epoch 6/100\n",
      "\u001b[1m3771/3771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 722us/step - loss: 0.0346 - mse: 0.0343 - val_loss: 0.0267 - val_mse: 0.0264\n",
      "Epoch 7/100\n",
      "\u001b[1m3771/3771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 753us/step - loss: 0.0346 - mse: 0.0343 - val_loss: 0.0267 - val_mse: 0.0264\n",
      "Epoch 8/100\n",
      "\u001b[1m3771/3771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 787us/step - loss: 0.0344 - mse: 0.0342 - val_loss: 0.0269 - val_mse: 0.0266\n",
      "Epoch 9/100\n",
      "\u001b[1m3771/3771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 793us/step - loss: 0.0344 - mse: 0.0342 - val_loss: 0.0268 - val_mse: 0.0265\n",
      "\u001b[1m2451/2451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 222us/step\n",
      "2009\n",
      "Epoch 1/100\n",
      "\u001b[1m3908/3908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 955us/step - loss: 0.0355 - mse: 0.0352 - val_loss: 0.0266 - val_mse: 0.0263\n",
      "Epoch 2/100\n",
      "\u001b[1m3908/3908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 854us/step - loss: 0.0354 - mse: 0.0351 - val_loss: 0.0266 - val_mse: 0.0263\n",
      "Epoch 3/100\n",
      "\u001b[1m3908/3908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 855us/step - loss: 0.0356 - mse: 0.0353 - val_loss: 0.0268 - val_mse: 0.0265\n",
      "\u001b[1m2195/2195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 253us/step\n",
      "2010\n",
      "Epoch 1/100\n",
      "\u001b[1m4041/4041\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 775us/step - loss: 0.0351 - mse: 0.0348 - val_loss: 0.0266 - val_mse: 0.0264\n",
      "Epoch 2/100\n",
      "\u001b[1m4041/4041\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 791us/step - loss: 0.0351 - mse: 0.0348 - val_loss: 0.0265 - val_mse: 0.0262\n",
      "Epoch 3/100\n",
      "\u001b[1m4041/4041\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 780us/step - loss: 0.0351 - mse: 0.0349 - val_loss: 0.0265 - val_mse: 0.0263\n",
      "Epoch 4/100\n",
      "\u001b[1m4041/4041\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 748us/step - loss: 0.0351 - mse: 0.0349 - val_loss: 0.0266 - val_mse: 0.0263\n",
      "\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 235us/step\n",
      "2011\n",
      "Epoch 1/100\n",
      "\u001b[1m4173/4173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 877us/step - loss: 0.0340 - mse: 0.0338 - val_loss: 0.0266 - val_mse: 0.0263\n",
      "Epoch 2/100\n",
      "\u001b[1m4173/4173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 754us/step - loss: 0.0340 - mse: 0.0338 - val_loss: 0.0265 - val_mse: 0.0263\n",
      "Epoch 3/100\n",
      "\u001b[1m4173/4173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 739us/step - loss: 0.0340 - mse: 0.0338 - val_loss: 0.0265 - val_mse: 0.0262\n",
      "Epoch 4/100\n",
      "\u001b[1m4173/4173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 738us/step - loss: 0.0340 - mse: 0.0338 - val_loss: 0.0265 - val_mse: 0.0262\n",
      "Epoch 5/100\n",
      "\u001b[1m4173/4173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 723us/step - loss: 0.0340 - mse: 0.0338 - val_loss: 0.0265 - val_mse: 0.0263\n",
      "\u001b[1m2105/2105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213us/step\n",
      "2012\n",
      "Epoch 1/100\n",
      "\u001b[1m4302/4302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 832us/step - loss: 0.0344 - mse: 0.0341 - val_loss: 0.0266 - val_mse: 0.0264\n",
      "Epoch 2/100\n",
      "\u001b[1m4302/4302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 747us/step - loss: 0.0344 - mse: 0.0341 - val_loss: 0.0266 - val_mse: 0.0264\n",
      "Epoch 3/100\n",
      "\u001b[1m4302/4302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 755us/step - loss: 0.0343 - mse: 0.0341 - val_loss: 0.0266 - val_mse: 0.0264\n",
      "Epoch 4/100\n",
      "\u001b[1m4302/4302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 808us/step - loss: 0.0343 - mse: 0.0341 - val_loss: 0.0266 - val_mse: 0.0264\n",
      "Epoch 5/100\n",
      "\u001b[1m4302/4302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 778us/step - loss: 0.0343 - mse: 0.0341 - val_loss: 0.0266 - val_mse: 0.0264\n",
      "Epoch 6/100\n",
      "\u001b[1m4302/4302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 799us/step - loss: 0.0343 - mse: 0.0341 - val_loss: 0.0266 - val_mse: 0.0264\n",
      "\u001b[1m2073/2073\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214us/step\n",
      "2013\n",
      "Epoch 1/100\n",
      "\u001b[1m4431/4431\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 783us/step - loss: 0.0352 - mse: 0.0350 - val_loss: 0.0267 - val_mse: 0.0265\n",
      "Epoch 2/100\n",
      "\u001b[1m4431/4431\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 822us/step - loss: 0.0352 - mse: 0.0350 - val_loss: 0.0266 - val_mse: 0.0264\n",
      "Epoch 3/100\n",
      "\u001b[1m4431/4431\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 816us/step - loss: 0.0352 - mse: 0.0350 - val_loss: 0.0266 - val_mse: 0.0264\n",
      "Epoch 4/100\n",
      "\u001b[1m4431/4431\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 809us/step - loss: 0.0352 - mse: 0.0350 - val_loss: 0.0266 - val_mse: 0.0264\n",
      "Epoch 5/100\n",
      "\u001b[1m4431/4431\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 853us/step - loss: 0.0352 - mse: 0.0350 - val_loss: 0.0265 - val_mse: 0.0263\n",
      "Epoch 6/100\n",
      "\u001b[1m4431/4431\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 791us/step - loss: 0.0352 - mse: 0.0350 - val_loss: 0.0266 - val_mse: 0.0264\n",
      "Epoch 7/100\n",
      "\u001b[1m4431/4431\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 780us/step - loss: 0.0352 - mse: 0.0350 - val_loss: 0.0265 - val_mse: 0.0262\n",
      "Epoch 8/100\n",
      "\u001b[1m4431/4431\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 840us/step - loss: 0.0353 - mse: 0.0350 - val_loss: 0.0265 - val_mse: 0.0263\n",
      "Epoch 9/100\n",
      "\u001b[1m4431/4431\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 803us/step - loss: 0.0353 - mse: 0.0351 - val_loss: 0.0265 - val_mse: 0.0263\n",
      "\u001b[1m2056/2056\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213us/step\n",
      "2014\n",
      "Epoch 1/100\n",
      "\u001b[1m4563/4563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 936us/step - loss: 0.0332 - mse: 0.0330 - val_loss: 0.0268 - val_mse: 0.0266\n",
      "Epoch 2/100\n",
      "\u001b[1m4563/4563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 812us/step - loss: 0.0332 - mse: 0.0330 - val_loss: 0.0268 - val_mse: 0.0266\n",
      "Epoch 3/100\n",
      "\u001b[1m4563/4563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 812us/step - loss: 0.0332 - mse: 0.0330 - val_loss: 0.0268 - val_mse: 0.0266\n",
      "\u001b[1m2115/2115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 235us/step\n",
      "2015\n",
      "Epoch 1/100\n",
      "\u001b[1m4698/4698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 832us/step - loss: 0.0334 - mse: 0.0332 - val_loss: 0.0268 - val_mse: 0.0266\n",
      "Epoch 2/100\n",
      "\u001b[1m4698/4698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 703us/step - loss: 0.0334 - mse: 0.0332 - val_loss: 0.0269 - val_mse: 0.0267\n",
      "Epoch 3/100\n",
      "\u001b[1m4698/4698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 776us/step - loss: 0.0334 - mse: 0.0332 - val_loss: 0.0269 - val_mse: 0.0267\n",
      "\u001b[1m2160/2160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 232us/step\n"
     ]
    }
   ],
   "source": [
    "end_year = date.max()\n",
    "\n",
    "total_R_2_OOS = [] \n",
    "model = nn2(lamda)\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "\n",
    "for i in range(1987 + 1, end_year.year):\n",
    "    print(i)\n",
    "    train_window = (date > datetime(1987,3,1)) & (date < datetime(i+1,1,1)) \n",
    "    test_window = (date >= datetime(i,1,1)) & (date < datetime(i+1,1,1)) \n",
    "    # print('Train', np.sum(train_window))\n",
    "    # print('Test', np.sum(test_window))\n",
    "    X_train_expanding, y_train_expanding = X.loc[train_window].values, y.loc[train_window].values\n",
    "    X_test_expanding, y_test_expanding =  X.loc[test_window].values, y.loc[test_window].values\n",
    "\n",
    "    # model.summary()\n",
    "    history = model.fit(X_train_expanding, y_train_expanding, \n",
    "                        epochs=100, \n",
    "                        batch_size=batch_size, \n",
    "                        verbose=True,\n",
    "                        validation_data = (X_val, y_val),\n",
    "                        #TODO \n",
    "                        # Defined the callback to see if the validation result actually improves \n",
    "                        callbacks = [EarlyStopping(patience = patience, restore_best_weights=True)])\n",
    "    predictions = model.predict(X_test_expanding)\n",
    "    df_predictions = pd.DataFrame(predictions, columns=['Prediction'])\n",
    "    df_predictions['Actual'] = y_test_expanding\n",
    "    df_predictions['dif_squared'] = (df_predictions['Prediction'] - df_predictions['Actual'])**2\n",
    "    df_predictions['actual_sqaured'] = df_predictions['Actual']**2\n",
    "    R_OOS = 1 - (df_predictions['dif_squared'].sum()/df_predictions['actual_sqaured'].sum()) \n",
    "    total_R_2_OOS.append(R_OOS)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012938800713457991"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(total_R_2_OOS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
