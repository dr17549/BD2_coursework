{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check requirements\n",
    "# !pip install cmake 'gym[atari]' scipy\n",
    "\n",
    "import gym \n",
    "from IPython.display import clear_output,display\n",
    "from time import sleep\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "data_path = '../data_offline'\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\").env \n",
    "env.reset()\n",
    "env.render()\n",
    "# rules: https://gym.openai.com/envs/Taxi-v3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random agent driving \n",
    "state , done = env.reset(), False\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    # random action\n",
    "    action = env.action_space.sample() \n",
    "    \n",
    "    # get new state and reward\n",
    "    newstate, reward, done, _ = env.step(action) \n",
    "    \n",
    "    # output\n",
    "    clear_output(wait=True)\n",
    "    env.render()\n",
    "    print('Experience:')\n",
    "    print('state : %d' % state)\n",
    "    print('action: %d' % action)\n",
    "    print('reward: %d' % reward)\n",
    "    print('next state: %d' % newstate)\n",
    "    sleep(5)\n",
    "    \n",
    "    state = newstate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\").env \n",
    "env.seed(34)\n",
    "\n",
    "Q = np.zeros((env.observation_space.n,env.action_space.n))\n",
    "\n",
    "epsilon = 0.1\n",
    "gamma = 0.9\n",
    "alpha = 0.1\n",
    "\n",
    "for episode in range(10000):\n",
    "    \n",
    "    if (episode+1) % 500 == 0:\n",
    "        print('episode %d' % (episode+1))\n",
    "    \n",
    "    state , done = env.reset(), False\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        if np.random.uniform() < 0.1:\n",
    "        \n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            action = None # TODO\n",
    "        \n",
    "        \n",
    "        # get new state and reward\n",
    "        newstate, reward, done, _ = env.step(action) \n",
    "\n",
    "        # update\n",
    "        y = reward + 0.9 * np.max(Q[newstate])\n",
    "    \n",
    "        Q[state,action] -= None # TODO\n",
    "\n",
    "        state = newstate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained agent driving\n",
    "env.seed(484) \n",
    "state , done = env.reset(), False\n",
    "while not done:    \n",
    "    # optimal action\n",
    "    action = np.argmax(Q[state])\n",
    "    \n",
    "    # get new state and reward\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    env.render()\n",
    "    sleep(.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portfolio problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path + '/mkt_rf_dp.csv')\n",
    "\n",
    "# excess return\n",
    "df['RetEx'] = df['Ret'] - df['Rfree']\n",
    "\n",
    "# lag dp ratio by a year\n",
    "df['D/P lag'] = df['D/P'].shift(12)\n",
    "\n",
    "# merton myopic strategy \n",
    "roll = np.log(1+df['Ret']).shift(1).rolling(240)\n",
    "mu, sigma2 = roll.mean(), roll.var()\n",
    "df['merton'] = (mu + 0.5*sigma2 - np.log(1+df['Rfree'])) / sigma2\n",
    "\n",
    "# binary signal\n",
    "roll = df['D/P lag'].rolling(240)\n",
    "df['signal'] = (df['D/P lag'] > roll.median()).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['signal'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=df,x='signal',y='RetEx',ci=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna().to_csv(data_path + '/merton_signal.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "class MertonLogSignal(gym.Env):\n",
    "\n",
    "    def __init__(self,horizon=12):\n",
    "        \n",
    "        # economic parameters\n",
    "        self.horizon = horizon \n",
    "        \n",
    "        # markov problem\n",
    "        self.observation_space=spaces.Discrete(2) \n",
    "        self.action_space=spaces.Discrete(3)\n",
    "        \n",
    "        # data\n",
    "        self.data = pd.read_csv(data_path + '/merton_signal.csv')  \n",
    "        \n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        # portfolio\n",
    "        self.merton = self.data['merton'].iloc[self.date]\n",
    "        self.port =  self.merton + (action-1) * 0.5\n",
    "        \n",
    "        # returns next period\n",
    "        Rf = self.data['Rfree'].iloc[self.date]\n",
    "        Re = self.data['RetEx'].iloc[self.date]\n",
    "\n",
    "        # log return\n",
    "        self.rp = np.log(1 + Rf + self.port * Re)\n",
    "        \n",
    "        # housekeeping\n",
    "        self.date += 1\n",
    "        self.life -= 1\n",
    "        self.dp = self.data['signal'].iloc[self.date]\n",
    "        \n",
    "        # output: state, reward, done, info\n",
    "        return self.dp, self.rp, self.life == 0, {}\n",
    "        \n",
    "\n",
    "    def reset(self):\n",
    "        # wealth\n",
    "        self.wealth = 100\n",
    "        \n",
    "        # time\n",
    "        self.date = np.random.choice(len(self.data)-self.horizon) # birthday \n",
    "        self.life = self.horizon # periods of life left\n",
    "        \n",
    "        # dp\n",
    "        self.dp = self.data['signal'].iloc[self.date]\n",
    "        \n",
    "        return self.dp\n",
    "        \n",
    "\n",
    "    def render(self):\n",
    "        print('Date: %s\\n' % self.data['yyyymm'].iloc[self.date])\n",
    "        print('Merton: %.2f\\n' % self.merton)\n",
    "        print('RoboMerton: %.2f\\n' % self.port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random agent investing\n",
    "env = MertonLogSignal()\n",
    "state , done = env.reset(), False\n",
    "for i in range(10):\n",
    "    \n",
    "    # random action\n",
    "    action = env.action_space.sample() \n",
    "    \n",
    "    # get new state and reward\n",
    "    state, reward, done, info = env.step(action) \n",
    "        \n",
    "    clear_output(wait=True)\n",
    "    env.render()\n",
    "    sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Q learning\n",
    "env = MertonLogSignal()\n",
    "env.seed(34)\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "for ep in range(50000):\n",
    "    \n",
    "    if (ep+1) % 10000 == 0:\n",
    "        print('episode %d' % (ep+1))\n",
    "        plt.figure()\n",
    "        plt.plot(Q.T)\n",
    "        plt.legend(['signal = 0','signal = 1'])\n",
    "        plt.title('Q after Episode %d\\n' % (ep+1))\n",
    "\n",
    "    \n",
    "    # initialise episode\n",
    "    state , done = env.reset(), False\n",
    "    \n",
    "    # set learning rate and exploration parameters\n",
    "    alpha = 1e-2 if ep < 10000 else 1e-3\n",
    "    epsilon = 1 if ep < 50000 else 0.1\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        # choose action using epsilon greedy\n",
    "        if np.random.uniform() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q[state])\n",
    "            \n",
    "        # draw new state and reward\n",
    "        newstate, reward, done, info = env.step(action) \n",
    "        \n",
    "        # target / update\n",
    "        y = reward + np.max(Q[newstate])\n",
    "        Q[state,action] -=  alpha * (Q[state,action]-y)\n",
    "        \n",
    "        state = newstate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
